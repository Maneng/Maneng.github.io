[{"title":"java类加载机制","date":"2017-04-07T06:39:29.810Z","path":"2017/04/07/java类加载机制/","text":"类加载过程 大致过程java中的类只有被JVM加载之后才能在程序中使用，加载的过程大致为加载-连接-初始化，其中连接又分为验证-准备-解析。所以细分为加载-验证-准备-解析-初始化五个阶段。其中加载、验证、准备和初始化这四个阶段发生的顺序是确定的，而解析阶段则不一定，有时为了动态绑定也会以在初始化阶段之后开始。类生命周期： 类加载的条件JVM肯定不会无缘无故就去加载.class，只有主动使用时才会初始化。主动使用有以下几种情况： 当创建一个类的实例时，比如使用new或者反射，克隆，反序列化。 当调用类的静态方法时，即使用了字节码invokestatic指令。 当时用类或接口的静态字段时(final常量除外)，即使用了字节码的getstatic和putstatic指令。 当使用了java.lang.reflect包中的方法反射类的方法时。 当初始化子类时，要求先初始化父类。 作为启动虚拟机，含有main()方法那个类。 12345678910111213141516public class Parent&#123; static&#123; System.out.println(&quot;parent init&quot;); &#125; public static int v=100;&#125;public class Child extends Parent&#123; static&#123; System.out.println(&quot;child init&quot;); &#125;&#125;public class Main&#123; public static void main(String args[])&#123; System.out.println(Child.v); &#125;&#125; 运行输出： 12parent init100 可以看到只有父类被初始化，子类没有。可见，引用一个子类时(例子中的v)，只有直接定义该字段的类(例子中的父类)，才会被初始化。注意，child虽然没有被初始化但是已经被加载。 类加载的过程-加载 目的：查找并加载类的二进制数据 加载一个类时，JVM需要完成的工作。 通过类的全名，获取类的二进制数据流。 解析类的二进制数据流为方法区内的数据结构。 创建java.lang.Class类的实例，表示该类型。 类加载的过程-验证 目的：确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 验证阶段是非常重要的，但不是必须的，它对程序运行期没有影响。如果所引用的类经过反复验证，那么可以考虑采用-Xverifynone参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。 验证阶段大致4个阶段： 文件格式验证验证字节流是否符合Class文件格式的规范；例如：是否以0xCAFEBABE开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。 元数据验证对字节码描述的信息进行语义分析（注意：对比javac编译阶段的语义分析），以保证其描述的信息符合Java语言规范的要求；例如：这个类是否有父类，除了java.lang.Object之外。 字节码验证通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。 符号引用验证确保解析动作能正确执行。 类加载的过程-准备 目的：为类的静态变量分配内存，并将其初始化为默认值 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存都在方法区中进行分配。 注意： 这时候进行内存分配的仅包括类变量，而不包括实例变量，实例变量会在对象实例化的时候随着对象一起分配在Java堆中； 这里所说的初始值“通常情况”下是数据类型的零值：假设一个类变量的定义为： public static int value = 123;那变量value在准备阶段过后的初始值为0而不是123，因为此时尚未开始执行任何Java方法，而把value赋值为123的putstatic指令是在程序被编译之后， 存放于类构造器方法中，所以把value赋值为123的动作将在初始化阶段才会执行。但是假设类变量value的定义为：public static final int value = 123;在准备阶段虚拟机就会根据ConstantValue的设置将value赋值为123。JAVA类变量默认值： 类加载的过程-解析目的： 把类中的符号引用转换为直接引用 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。 符号引用： 符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可。符号引用与虚拟机实现的内存布局无关，引用的目标并不一定已经加载到内存当中。各种虚拟机实现的内存布局可以各不相同，但是它们能接受的符号引用必须都是一致的，因为符号引用的字面量形式明确定义在Java虚拟机规范的Class文件格式中。 直接引用: 直接引用可以是直接指向目标的指针、相对偏移量或者是一个能简介定位到目标的句柄。直接引用是和虚拟机实现的内存布局息息相关的，同一个符号引用在不同虚拟机实例上翻译出来的直接引用一般不会相同。如果有了直接引用，那引用的目标必定已经在内存中存在。 类加载的过程-初始化除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作均由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中定义的Java程序代码（或者说是字节码）。初始化阶段是执行类构造器clinit方法的过程。在Java中对类变量进行初始值设定有两种方式：①声明类变量是指定初始值②使用静态代码块为类变量指定初始值。clinit方法 由编译器收集类中的所有类变量的赋值动作（如果仅仅只是声明，不会被收集）和静态语句块中的语句合并产生的，收集顺序按照语句在源文件中出现的顺序所决定；在静态语句块中只能访问定义在静态语句之前的变量；而对于定义在静态语句块之后的变量，可以进行赋值，但是不能够访问。 不需要显示调用父类构造器，虚拟机会保证在子类的clinit()方法执行之前，父类的clinit()方法已经执行完毕，所以，第一个被执行的clinit()方法的类肯定是java.lang.Object。 父类中定义的静态语句块优先于子类的静态语句。 此方法对类和接口都不是必须的，若类中没有静态语句块和静态变量赋值操作，则不会生成clinit()方法。 接口会生成此方法，因为对接口的字段可以进行赋值操作。执行接口的clinit()方法不需要先执行父接口的clinit()方法，只有在使用父接口的变量时，才会进行初始化；接口的实现类在初始化时也不会执行接口的clinit()方法。 此方法在多线程环境中会被正确的加锁、同步。 类加载器 类加载器种类1. Bootstrap ClassLoader(启动类加载器)一般由C++实现，是虚拟机的一部分。该类加载器主要职责是将JAVA_HOME路径下的\\lib目录中能被虚拟机识别的类库(比如rt.jar)加载到虚拟机内存中。Java程序无法直接引用该类加载器2. Extension ClassLoader(扩展类加载器)由Java实现，独立于虚拟机的外部。该类加载器主要职责将JAVA_HOME路径下的\\lib\\ext目录中的所有类库，开发者可直接使用扩展类加载器。 该加载器是由sun.misc.Launcher$ExtClassLoader实现。3 Application ClassLoader(应用程序类加载器)该加载器是由sun.misc.Launcher$AppClassLoader实现，该类加载器负责加载用户类路径上所指定的类库。开发者可通过ClassLoader.getSystemClassLoader()方法直接获取，故又称为系统类加载器。当应用程序没有自定义类加载器时，默认采用该类加载器。 双亲委托模式 如果一个类加载器收到了类加载请求，他不会尝试自己去加载，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有类加载的请求最终都应该传到顶层的启动类加载器中，只有当父类加载器反馈自己无法完成这个类加载的请求时，子类才会尝试自己加载。但也有弊端， 顶层的ClassLoader无法访问底层的ClassLoader所加载的类所造成的问题。 使用双亲委托模型来组织类加载器之间的关系，有一个显而易见的好处就是Java类随着它的类加载器一起具备了一种带有优先级的层次关系，例如java.lang.Object存放在rt.jar之中，无论那个类加载器要加载这个类，最终都是委托给启动类加载器进行加载，因此Object类在程序的各种类加载器环境中都是同一个类，相反，如果没有双亲委托模型，由各个类加载器去完成的话，如果用户自己写一个名为java.lang.Object的类，并放在classpath中，应用程序中可能会出现多个不同的Object类，java类型体系中最基本安全行为也就无法保证。 java.lang.ClassLoaderjava.lang.ClassLoader类的基本职责就是根据一个指定的类的名称，找到或者生成其对应的字节代码，然后从这些字节代码中定义出一个Java类,还负责加载 Java 应用所需的资源，如图像文件和配置文件等.ClassLoader 中与加载类相关的方法: | 方法 | 说明 || ————– | ———— || getParent() |返回该类加载器的父类加载器。|| loadClass(String name) | 加载名称为 name 的类，返回的结果是 java.lang.Class 类的实例。|| findClass(String name) | 查找名称为 name 的类，返回的结果是 java.lang.Class 类的实例。|| findLoadedClass(String name) | 查找名称为 name 的已经被加载过的类，返回的结果是 java.lang.Class 类的实例。|| defineClass(String name, byte[] b, int off, int len) | 把字节数组 b 中的内容转换成 Java 类，返回的结果是 java.lang.Class 类的实例。这个方法被声明为 final 的。|| resolveClass(Class&lt;?&gt; c) | 链接指定的 Java 类。| 自定义类加载器定义类加载器可以选择 继承ClassLoader类，重写里面的方法来实现。loadClass()方法重写的话可能会破坏双亲委托模型，不推荐重写；defineClass：主要用于将原始字节转换为Class对象，不需要重写；findClass：根据名称来查找类，一般就重写这个方法。12345678910111213141516171819202122232425262728293031323334 public class Main &#123; static class MyClassLoader extends ClassLoader &#123; private String classPath; public MyClassLoader(String classPath) &#123; this.classPath = classPath; &#125; private byte[] loadByte(String name) throws Exception &#123; name = name.replaceAll(&quot;\\\\.&quot;, &quot;/&quot;); FileInputStream fis = new FileInputStream(classPath + &quot;/&quot; + name + &quot;.class&quot;); int len = fis.available(); byte[] data = new byte[len]; fis.read(data); fis.close(); return data; &#125; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; try &#123; byte[] data = loadByte(name); return defineClass(name, data, 0, data.length); &#125; catch (Exception e) &#123; e.printStackTrace(); throw new ClassNotFoundException(); &#125; &#125; &#125;; public static void main(String args[]) throws Exception &#123; MyClassLoader classLoader = new MyClassLoader(&quot;D:/test&quot;); Class clazz = classLoader.loadClass(&quot;com.huachao.cl.Test&quot;); Object obj = clazz.newInstance(); Method helloMethod = clazz.getDeclaredMethod(&quot;hello&quot;, null); helloMethod.invoke(obj, null); &#125;&#125; ##参考 《实战java虚拟机》 http://www.jianshu.com/p/11cc2de9dbc2 http://www.cnblogs.com/leesf456/p/5269545.html http://www.jsondream.com/2016/11/16/jvm-class-load-parent-Delegate.html https://www.ibm.com/developerworks/cn/java/j-lo-classloader/#download http://www.jianshu.com/p/acc7595f1b9d?utm_source=tuicool&amp;utm_medium=referral","tags":[{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"}]},{"title":"java注解总结","date":"2017-04-07T06:33:42.787Z","path":"2017/04/07/java注解总结/","text":"##注解分类 源码注解： 只在源码中存在，编译成.class就不存在了。 编译时注解： 源码和.class中都存在，比如@Override，@Deprecated等 运行时注解： 在运行时注解，比如spring的@Component，@AutoWired等 ##标准注解 注解标识 作用 @Override 覆盖超类中的方法 @Deprecated 声明过期 @SuppressWarnings 关闭不当的编译器警告信息 自定义注解 #####元注解 用于自定义注解 元注解类型 @Target @Retention @Documented @Inherited @Target 定义注解将应用于什么地方,当注解未指定Target值时，此注解可以使用任何元素之上,取值(ElementType)有: TYPE: 接口、类、枚举、注解 FIELD: 字段、枚举的常量 METHOD: 方法 PARAMETER: 方法参数 CONSTRUCTOR: 构造器 LOCAL_VARIABLE: 局部变量 ANNOTATION_TYPE: 注解类型 PACKAGE: 包 TYPE_PARAMETER: 类型参数声明(1.8新增)。 比如public class MyList&lt;@MySet T&gt; {}，在定义@MySet，必须在MySet的@Target设置 ElementType.TYPE_PARAMETER ，表示这个注解可以用来标注类型参数。 TYPE_USE: 类型使用声明(1.8新增)。只要是类型名称，都可以进行注解，以下的使用注解都是可以的: 123456List&lt;@Test Comparable&gt; list1 = new ArrayList&lt;&gt;();List&lt;? extends Comparable&gt; list2 = new ArrayList&lt;@Test Comparable&gt;();@Test String text;text = (@Test String)new Object();java.util. @Test Scanner console;console = new java.util.@Test Scanner(System.in); @Retention 定义注解在哪一个级别可用，当注解未定义Retention值时，默认值是CLASS，在源代码中，类文件中或者运行时，取值(RetentionPoicy)有： SOURCE ：注解将被编译器丢弃（该类型的注解信息只会保留在源码里，源码经过编译后，注解信息会被丢弃，不会保留在编译好的class文件里) CLASS ：注解在class文件中可用，但会被JVM丢弃（该类型的注解信息会保留在源码里和class文件里，在执行的时候，不会加载到虚拟机（JVM）中） RUNTIME ：JVM将在运行期也保留注解信息，因此可以通过反射机制读取注解的信息（源码、class文件和执行的时候都有注解的信息） @Documented 用于描述其它类型的annotation应该被作为被标注的程序成员的公共API，因此可以被例如javadoc此类的工具文档化。是一个标记注解，没有成员。 @Inherited 阐述了某个被标注的类型是被继承的。如果一个使用了@Inherited修饰的annotation类型被用于一个class，则这个annotation将被用于该class的子类。是一个标记注解，没有成员。 ####示例 12345678@Target(&#123;ElementType.METHOD,ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface Desc &#123; String value(); String desc() default &quot;description：&quot;;&#125; 注意事项 注解可以设定初始值，使用default就可以实现。 注解只有一个元素的时候，该元素名称必须是value，并且在使用该注解的时候可以省略”value=”。 注解的值必须是确定的，且不能使用null作为值。 注解可能的类型： 基本类型（int,float,boolean,byte,double,char,long,short） String Class enum Annotation 以上类型的数组 如果使用了其他类型，那编译器就会报错。也不允许使用任何包装类型。注解也可以作为元素的类型，也就是注解可以嵌套。 元素的修饰符，只能用 public 或 default ####编写注解解析器 要想注添加逻辑，需要反射或者字节码操作获取注解信息 被注解的测试类 123@Desc(value = &quot;test&quot;,desc = &quot;Test is a type&quot;) public class Test &#123;&#125; 反射获取注解信息 123456789101112 public class Main &#123; public static void main(String[] args) throws Exception&#123; Class&lt;?&gt; c=Class.forName(&quot;me.jcala.tip.annotation.Test&quot;); Annotation annotation=c.getAnnotation(Desc.class); if (annotation!=null)&#123; Desc desc=(Desc)annotation; System.out.println(&quot;value:&quot;+desc.value()); System.out.println(&quot;description:&quot;+desc.desc()); &#125; &#125; &#125;&#125; #####参考: 慕课网 java编程思想 http://www.open-open.com/lib/view/open1453426805042.html","tags":[{"name":"java,注解","slug":"java-注解","permalink":"http://yoursite.com/tags/java-注解/"}]},{"title":"volatile 学习","date":"2017-04-07T06:32:41.368Z","path":"2017/04/07/volatile 学习/","text":"volatile ###作用: 保证变量对所有线程的可见性，即一个线程修改了某个值，新值对其他线程来说是立即可见的。(强制从公共堆栈中取得变量的值，而不是从线程私有数据栈中取得变量的值。) Java内存模型规定，对于多个线程共享的变量，存储在主内存当中，每个线程都有自己独立的工作内存（比如CPU的寄存器），线程只能访问自己的工作内存，不可以访问其它线程的工作内存。工作内存中保存了主内存共享变量的副本，线程要操作这些共享变量，只能通过操作工作内存中的副本来实现，操作完毕之后再同步回到主内存当中。volatile保在每次访问变量时都会进行一次刷新，因此每次访问都是主内存中最新的版本。 禁止指令重排序 指令重排序是JVM为了优化指令，提高程序运行效率，在不影响单线程程序执行结果的前提下，尽可能地提高并行度。但是指令重排序在多线程时会出现一些问题。如下： 1234567891011121314在线程A中:context = loadContext();inited = true;在线程B中:while(!inited )&#123; //根据线程A中对inited变量的修改决定是否使用context变量 sleep(100);&#125;doSomethingwithconfig(context);假设线程A中发生了指令重排序:inited = true;context = loadContext();那么B中很可能就会拿到一个尚未初始化或尚未初始化完成的context,从而引发程序错误。 而volatile使用内存屏障可以禁用指令重排序，避免这种问题。 ###与synchronized对比: volatile可以保证数据的可见性，但不可以保证原子性 synchronized可以保证原子性，也可以间接保证可见性 synchronized有volatile同步的功能 volatile性能一般高于synchronized 正确使用 volatile 变量的条件 对变量的写操作不依赖于当前值。(不能是自增自减等操作) 该变量没有包含在具有其他变量的不变式中。包含在具有其他变量的不变式中的情况(包含了一个不变式 —— 下界总是小于或等于上界)：123456789101112131415public class NumberRange &#123; private int lower, upper; public int getLower() &#123; return lower; &#125; public int getUpper() &#123; return upper; &#125; public void setLower(int value) &#123; if (value &gt; upper) throw new IllegalArgumentException(&quot;&quot;); lower = value; &#125; public void setUpper(int value) &#123; if (value &lt; lower) throw new IllegalArgumentException(&quot;&quot;); upper = value; &#125;&#125; 如果凑巧两个线程在同一时间使用不一致的值执行 setLower 和 setUpper 的话，则会使范围处于不一致的状态.例如，如果初始状态是(0, 5)，同一时间内，线程 A 调用 setLower(4) 并且线程 B 调用 setUpper(3)，显然这两个操作交叉存入的值是不符合条件的，那么两个线程都会通过用于保护不变式的检查，使得最后的范围值是 (4, 3) —— 一个无效值。 ###volatile可以解决的问题示例: 123456789101112131415161718192021222324252627282930public class RunThread extends Thread&#123; private boolean isRunning=true; public boolean isRunning()&#123; return isRunning; &#125; public void setRunning(boolean isRunning)&#123; this.isRunning=isRunning; &#125; @Override public void run()&#123; System.out.println(&quot;进入run了&quot;); while (isRunning==true)&#123; &#125; System.out.println(&quot;线程被停止了&quot;); &#125;&#125;public class Main&#123; public static void main(String args[])&#123; try&#123; RunThread thread=new RunThread(); thread.start(); Thread.sleep(1000); thread.setRunning(false); System.out.println(&quot;已经赋值为false&quot;); &#125;catch (InterruptedException e)&#123; e.printStackTrace(); &#125; &#125;&#125; 运行时加入JVM参数, -server。查看结果发现System.out.println(“线程被停止了”);从未执行。 因为JVM被设置为-server模式时为了线程的运行效率，线程一直在私有堆栈中取得isRunning的值为true，而更新的却是公共堆栈中的isRunning。 解决方法：volatile private boolean isRunning=true; ###线程工作内存和主工作内存之间通过8中原子操作实现 lock（锁定）：作用于主内存的变量，它把一个变量标识为一条线程独占的状态。 unlock（解锁）：作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其它线程锁定。 read（读取）：作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的load动作使用。 load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use（使用）：作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用到变量的值的字节码指令时将会执行这个操作。 assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。 store（存储）：作用于工作内存的变量，它把工作内存中一个变量的值传送给主内存中，以便随后的write操作使用。 write（写入）：作用于主内存的变量，它把store操作从工作内存中得到的变量的值放入主内存的变量中。 ***对于volatile修饰的变量，只能保证从主内存到工作内存的值是最新的，但像use和assign这些操作并不是原子性的，因此volatile无法保证原子性*** ###使用原子类进行i++操作 123456789101112131415161718192021import java.util.concurrent.atomic.AtomicInteger; public class AddCountThread extends Thread&#123; private AtomicInteger count=new AtomicInteger(0); @Override public void run()&#123; for (int i=0;i&lt;1000;i++)&#123; System.out.println(count.incrementAndGet()); &#125; &#125;&#125;public class Main&#123; public static void main(String args[])&#123; AddCountThread thread=new AddCountThread(); new Thread(thread).start(); new Thread(thread).start(); new Thread(thread).start(); new Thread(thread).start(); new Thread(thread).start(); &#125;&#125; ###原子类也不完全安全 123456789101112131415161718192021222324252627282930313233343536373839import java.util.concurrent.atomic.AtomicLong;public class MyService&#123; public static AtomicLong aiRef=new AtomicLong(); public void addNum()&#123; System.out.println(Thread.currentThread().getName()+&quot;加了100后的值是:&quot;+aiRef.addAndGet(100)); aiRef.addAndGet(1); &#125;&#125;public class MyThread extends Thread&#123; private MyService myService; public MyThread(MyService myService)&#123; super(); this.myService=myService; &#125; @Override public void run()&#123; myService.addNum(); &#125;&#125;public class Main&#123; public static void main(String args[])&#123; try&#123; MyService service=new MyService(); MyThread[] myThreads=new MyThread[5]; for (int i=0;i&lt;myThreads.length;i++)&#123; myThreads[i]=new MyThread(service); &#125; for (int i=0;i&lt;myThreads.length;i++)&#123; myThreads[i].start(); &#125; Thread.sleep(1000); System.out.println(service.aiRef.get()); &#125;catch (InterruptedException e)&#123; e.printStackTrace(); &#125; &#125;&#125; 运行发现打印出错，这是因为addAndGet()方法虽然是原子性的，但方法与方法之间的调用不是原子性的。 #####参考: JAVA多线程编程核心技术 深入理解Java虚拟机 http://www.codeceo.com/article/jvm-memory-model-visual.html?utm_source=tuicool&amp;utm_medium=referral http://www.importnew.com/23535.html?utm_source=tuicool&amp;utm_medium=referral http://www.jianshu.com/p/03a8f06ede46?utm_source=tuicool&amp;utm_medium=referral","tags":[{"name":"java,volatile","slug":"java-volatile","permalink":"http://yoursite.com/tags/java-volatile/"}]},{"title":"JPA参考手册","date":"2017-04-07T06:31:15.800Z","path":"2017/04/07/JPA参考手册/","text":"1. JPA普通注解 @Entity: 声明为一个实体。(修饰实体类) @Table: 指定实体所映射的表。(修饰实体类) 属性 是否必要 说明 name 否 设置实体映射的表名。不指定则与实体类的类名相同 catalog 否 设置实体映射的表放入指定的catalog中。不指定则放入默认的catalog中 schema 否 设置实体映射的表放入指定的schema中。 不指定则放入默认的schema中 uniqueConstraints 否 为实体映射的表设置唯一的约束。该属性可以是一个@UniqueConstraint Annotation数组 @Indexed：定义索引 12345678910111213141516@Entity@Table(name=&quot;person_table&quot;, indexes = &#123; @Index(name=&quot;idx_person&quot;, columnList=&quot;name&quot;) &#125;)@NamedQuery( name=&quot;simpleByTest&quot;, query=&quot;SELECT x FROM SimpleModel x WHERE x.test LIKE :test&quot; )public class Person &#123; @Id @GeneratedValue private Long id; private String name; //...&#125; @secondaryTable: 把实体的部分属性映射到第二个数据表。可通过@secondaryTable指定多个额外的数据表。(修饰实体类) | 属性 | 是否必要 | 说明 | | ————– | ———— | ———— | | name | 否 | 指定新数据表的表名 | | catalog | 否 | 设置实体映射的表放入指定的catalog中。不指定则放入默认的catalog中 | | schema | 否 | 设置实体映射的表放入指定的schema中。 不指定则放入默认的schema中 | | uniqueConstraints| 否 | 为实体映射的表设置唯一的约束。 | | pkJoinColumns| 否 | 指定新数据表的一个或多个外键列，只有通过该外键列才可让新数据表中的记录参照到主表记录。该属性值是一个@PrimaryKeyJoinColumn数组 | @PrimaryKeyJoinColumn: 用于定义在从表中定义的外键列的映射信息。 | 属性 | 是否必要 | 说明 | | ————– | ———— | ———— | | name | 否 | 指定从表中的外键列的列名 | | columnDefinition | 否 | 指定JPA使用该属性值指定的SQL片段来创建外键列 | | referencedColumnName| 否 | 指定从表中外键参照的数据列的列名 | 1234567891011121314@Entity@Table(name=&quot;person_table&quot;)@SecondaryTable(name=&quot;person_detail&quot;,pkJoinColumns=@PrimaryKeyJoinColumn(name=&quot;person_id&quot;))public class Person&#123;@Idprivate int id;@Column(name=&quot;person_name&quot;,length=50)private String name;@Column(table=&quot;person_detail&quot;,name=&quot;email&quot;)private String email;@Column(table=&quot;person_detail&quot;,name=&quot;phone&quot;)private String phone;&#125;指定将实体状态放入第二个person_detail数据表中，并指定email,phone两个属性放入person_detail数据表中 @Column: 指定属性映射的列信息，如列名，长度等。(修饰属性) | 属性 | 是否必要 | 说明 | | ————– | ———— | ———— | | name | 否 | 指定该列的列名。默认为属性名 | | length | 否 | 指定该列所能保存的数据的最大长度。默认为255 | | nullable | 否 | 指定该列是否允许为null。默认为true | | unique | 否 | 指定该列是否具有唯一约束。默认为false | | updatable | 否 | 指定该列是否包含在JPA生成的update语句的列列表中。默认为true | | insertable | 否 | 指定该列是否包含在JPA生成的insert语句的列列表中。默认为true | @Id和@GeneratedValue：映射实体类的主键。(修饰属性)@Id定义主键，可以是基本类型，基本类型的包装类，String，Date等类型。@GeneratedValue：设置自动生成属性值，属性如下： | 属性 | 是否必要 | 说明 | | ————– | ———— | ———— | | strategy | 否 | 使用怎样的主键生成策略。GenerationType.AUTO：JPA自动选择最适合底层数据库的主键生成策略; GenerationType.IDENTITY: 对于mysql，sqlserver这样的数据库选择自增长的主键生成策略；GenerationType.SEQUENCE: 对于oracle这样的数据库，选择使用基于sequence的主键生成策略，应与@SequenceGenderator一起使用；GenerationType.TABLE：使用一个辅助表来生成主键，应与@TableGenderator一起使用| | generator| 否 | 当使用GenerationType.SEQUENCE,GenerationType.TABLE主键生成策略时，该属性指定sequence,辅助表的名称 | @Transient: 修饰不想持久保存的属性。(修饰属性) @Enumerated： 修饰枚举类型。(修饰属性) 123当@Enumerated的value属性为EnumType.STRING时，底层数据库保存的是枚举值的名称；当@Enumerated的value属性为EnumType.ORDINAL时，保存枚举值的序号。如@Enumerated(EnumType.ORDINAL). @Lob:修饰大数据类型，对应JDBC的java.sql.Clob类型或者java.sql.Blob类型。(修饰属性) 12当修饰的属性为byte[],Byte[],java.io.Serializable类型时，将映射为数据库底层的Blob列；当修饰的属性为char[],Character[]或java.lang.String类型时，映射为底层的Clob列。 @Basic：用于延迟加载操作。(修饰属性) 12345678比如JPA加载Person实体时并不需要立即加载它的pic属性，而只加载一个&quot;虚拟的&quot;代理，真正需要pic属性再从数据库加载。@Basic可以指定的属性：fetch:指定是否需要延迟加载该属性。FetchType.EAGER不使用延迟加载，Fetch.LAZY使用延迟加载。optional：指定映射的数据列是否允许使用null值。例如：@Lob@Basic(fetch=FetchType.LAZY)private byte[] pic; @Temporal: 修饰日期类型。(修饰属性) 123@Temporal可以指定一个value属性，该属性支持Temporal.DATE,Temporal.TIME,Temporal.TIMESTAMP，分别对应于数据库date,time,timestamp类型的数据列。 @Embedded和@Embeddable: 映射复合类型。@Embeadded修饰这个复合类型属性，@Embeaddable修饰这个复合类。@AttributeOverride用来指定复合类型的成员属性的映射配置，它支持的属性： | 属性 | 是否必要 | 说明 | | ————– | ———— | ———— | | name | 是 | 指定对复合类型中哪个属性进行配置 | | column | 是 | 指定该属性所映射的数据列的列名 | 12345678910111213141516171819@Entity@Table(name=&quot;person_table&quot;)public class Person&#123;@Idprivate int id;@Column(name=&quot;person_name&quot;,length=50)private String name;@Embedded@AttributeOverrides(&#123; @AttributeOverride(name=&quot;name&quot;,column=@Column(name=&quot;cat_name&quot;,length=35)), @AttributeOverride(name=&quot;color&quot;,column=@Column(&quot;cat_color&quot;))&#125;)private Cat cat;&#125;@Embeddablepublic class Cat&#123; private String name; private String color;&#125; @IdClass和@EmbeddedId: 定义复合类型的主键。(修饰属性)定义复合类型的主键有两种方式：(1). 使用@IdClass和多个@Id;(2). 使用一个@EmbeddedId即可。 123456789101112131415161718192021222324252627282930313233方式一：使用@IdClass和多个@Id @Entity @Table(name=&quot;person_table&quot;) @IdClass(Cat.class) public class Person&#123; //两个@Id定义联合主键 @Id private int id; @Id private String name; private Cat cat; &#125;方式二：用一个@EmbeddedId @Entity @Table(name=&quot;person_table&quot;) public class Person&#123; @Id private int id; @Column(name=&quot;person_name&quot;,length=50) private String name; @EmbeddedId @Embedded @AttributeOverrides(&#123; @AttributeOverride(name=&quot;name&quot;,column=@Column(name=&quot;cat_name&quot;,length=35)), @AttributeOverride(name=&quot;color&quot;,column=@Column(&quot;cat_color&quot;)) &#125;) private Cat cat; &#125; @Embeddable public class Cat&#123; private String name; private String color; &#125; @OrderBy: 对关联实体进行排序 123456789101112131415161718192021222324252627282930313233//Example 1: @Entity public class Course &#123; @ManyToMany @OrderBy(&quot;lastname ASC&quot;) public List&lt;Student&gt; getStudents() &#123;&#125;; &#125; //Example 2: @Entity public class Student &#123; @ManyToMany(mappedBy=&quot;students&quot;) @OrderBy // ordering by primary key is assumed public List&lt;Course&gt; getCourses() &#123;&#125;; &#125; //Example 3: @Entity public class Person &#123; @ElementCollection @OrderBy(&quot;zipcode.zip, zipcode.plusFour&quot;) public Set&lt;Address&gt; getResidences() &#123;&#125;; &#125; @Embeddable public class Address &#123; protected String street; protected String city; protected String state; @Embedded protected Zipcode zipcode; &#125; @Embeddable public class Zipcode &#123; protected String zip; protected String plusFour; &#125; @JoinColumn: 定义外键。(修饰属性) | 属性 | 是否必要 | 说明 | | ————– | ———— | ———— | | columnDefinition | 否 | 指定JPA使用该属性值指定的SQL片段来创建外键列 | | name | 否 | 指定该外键列的列名 | | insertable | 否 | 指定该列是否包含在JPA生成的insert语句的列列表中。默认为true | | updatable | 否 | 指定该列是否包含在JPA生成的update语句的列列表中。默认为true | | nullable | 否 | 指定该列是否允许为null。默认为true | | table | 否 | 指定该列所在的数据表的表名 | | unique | 否 | 指定是否为该列增加唯一约束 | | referenceColumnName | 否| 指定该列所参照的主键列的列名 | @ManyToOne: 映射多对一关系。(修饰属性) | 属性 | 是否必要 | 说明 | | ————– | ———— | ———— | | cascade | 否 | 指定JPA对关联实体采用怎样的级联策略，该级联策略支持四个属性值。CascadeType.ALL：指定JPA将所有的持久化操作都级联到关联实体；CascadeType.MERGE: 指定JPA将merge操作都级联到关联实体；CascadeType.PERSIST：指定JPA将persist操作级联到关联实体；CascadeType.REFRESH: 指定JPA将refresh操作级联到关联实体；CascadeType.REMOVE: 指定JPA将remove操作关联到关联实体| | fetch | 否 | 指定抓取关联实体时抓取策略，该属性支持两个值。FetchType.EAGER: 抓取实体时，立即抓取关联实体，默认值；FetchType.LAZY：抓取实体时延迟抓取关联实体，等到真到用到时再去抓取。 | | optional | 否 | 该属性指定关联关系是否可选。 | | targetEntity| 否 | 该属性指定关联实体的类名。 在默认情况下，JPA通过反射判断 | @OneToOne: 映射一对一关系。(修饰属性) | 属性 | 是否必要 | 说明 | | ————– | ———— | ———— | | cascade | 否 | 指定JPA对关联实体采用怎样的级联策略，该级联策略支持四个属性值。CascadeType.ALL：指定JPA将所有的持久化操作都级联到关联实体；CascadeType.MERGE: 指定JPA将merge操作都级联到关联实体；CascadeType.PERSIST：指定JPA将persist操作级联到关联实体；CascadeType.REFRESH: 指定JPA将refresh操作级联到关联实体；CascadeType.REMOVE: 指定JPA将remove操作关联到关联实体| | fetch | 否 | 指定抓取关联实体时抓取策略，该属性支持两个值。FetchType.EAGER: 抓取实体时，立即抓取关联实体，默认值；FetchType.LAZY：抓取实体时延迟抓取关联实体，等到真到用到时再去抓取。 | | optional | 否 | 该属性指定关联关系是否可选。 | | targetEntity| 否 | 该属性指定关联实体的类名。 在默认情况下，JPA通过反射判断 | | mappedBy | 否 | 该属性合法的属性值为关联实体的属性名，该属性指定关联实体中哪一个属性可引用到关联实体时采取抓取。 | @OneToMany：映射一对多关系。(修饰属性) | 属性 | 是否必要 | 说明 | | ————– | ———— | ———— | | cascade | 否 | 指定JPA对关联实体采用怎样的级联策略，该级联策略支持四个属性值。CascadeType.ALL：指定JPA将所有的持久化操作都级联到关联实体；CascadeType.MERGE: 指定JPA将merge操作都级联到关联实体；CascadeType.PERSIST：指定JPA将persist操作级联到关联实体；CascadeType.REFRESH: 指定JPA将refresh操作级联到关联实体；CascadeType.REMOVE: 指定JPA将remove操作关联到关联实体| | fetch | 否 | 指定抓取关联实体时抓取策略，该属性支持两个值。FetchType.EAGER: 抓取实体时，立即抓取关联实体，默认值；FetchType.LAZY：抓取实体时延迟抓取关联实体，等到真到用到时再去抓取。 | | targetEntity| 否 | 该属性指定关联实体的类名。 在默认情况下，JPA通过反射判断 | | mappedBy | 否 | 该属性合法的属性值为关联实体的属性名，该属性指定关联实体中哪一个属性可引用到关联实体时采取抓取。 | @ManyToMany：映射多对多关系。(修饰属性) | 属性 | 是否必要 | 说明 | | ————– | ———— | ———— | | cascade | 否 | 指定JPA对关联实体采用怎样的级联策略，该级联策略支持四个属性值。CascadeType.ALL：指定JPA将所有的持久化操作都级联到关联实体；CascadeType.MERGE: 指定JPA将merge操作都级联到关联实体；CascadeType.PERSIST：指定JPA将persist操作级联到关联实体；CascadeType.REFRESH: 指定JPA将refresh操作级联到关联实体；CascadeType.REMOVE: 指定JPA将remove操作关联到关联实体| | fetch | 否 | 指定抓取关联实体时抓取策略，该属性支持两个值。FetchType.EAGER: 抓取实体时，立即抓取关联实体，默认值；FetchType.LAZY：抓取实体时延迟抓取关联实体，等到真到用到时再去抓取。 | | targetEntity| 否 | 该属性指定关联实体的类名。 在默认情况下，JPA通过反射判断 | | mappedBy | 否 | 该属性合法的属性值为关联实体的属性名，该属性指定关联实体中哪一个属性可引用到关联实体时采取抓取。 | @JoinTable：专门用于多对多关联关系指定连接表的配置信息。 | 属性 | 是否必要 | 说明 | | ————– | ———— | ———— | | name | 否 | 指定该连接表的表名 | | catalog | 否 | 设置将该连接表放入指定的catalog内。如果没有指定该属性，连接表放入默认的catalog中。| | schema | 否 | 设置将该连接表放入指定的schema内。 如果没有指定该属性，连接表放入默认的schema中。 | | joinColumns | 否 | 该属性值可接受多个@JoinColumn，用于配置连接表中外键列的列信息，这些列参照当前实体对应表的主键列 | | inverseJoinColumns | 否 |该属性值可接受多个@JoinColumn，用于配置连接表中外键列的列信息，这些列参照当前实体的关联实体对应表的主键列 | | uniqueConstraints | 否 | 该属性为连接表增加唯一约束。 | @MapKey: 使用Map集合记录关联实体。 MappedSuperClass: 映射为非实体父类，该实体父类不会生成对应的数据表 12345678910111213141516171819202122232425262728293031323334353637@MappedSuperclass public class Employee &#123; @Id protected Integer empId; @Version protected Integer version; @ManyToOne @JoinColumn(name=&quot;ADDR&quot;) protected Address address; public Integer getEmpId() &#123; &#125; public void setEmpId(Integer id) &#123; &#125; public Address getAddress() &#123; &#125; public void setAddress(Address addr) &#123; &#125; &#125; // Default table is FTEMPLOYEE table @Entity public class FTEmployee extends Employee &#123; // Inherited empId field mapped to FTEMPLOYEE.EMPID // Inherited version field mapped to FTEMPLOYEE.VERSION // Inherited address field mapped to FTEMPLOYEE.ADDR fk // Defaults to FTEMPLOYEE.SALARY protected Integer salary; public FTEmployee() &#123;&#125; public Integer getSalary() &#123; &#125; public void setSalary(Integer salary) &#123; &#125; &#125; @Entity @Table(name=&quot;PT_EMP&quot;) @AssociationOverride( name=&quot;address&quot;, joincolumns=@JoinColumn(name=&quot;ADDR_ID&quot;)) public class PartTimeEmployee extends Employee &#123; // Inherited empId field mapped to PT_EMP.EMPID // Inherited version field mapped to PT_EMP.VERSION // address field mapping overridden to PT_EMP.ADDR_ID fk @Column(name=&quot;WAGE&quot;) protected Float hourlyWage; public PartTimeEmployee() &#123;&#125; public Float getHourlyWage() &#123;&#125; public void setHourlyWage(Float wage) &#123;&#125; &#125; @Inheritance：指定映射策略 InheritanceType.SINGLE_TABLE： 整个类层次对应一张表策略,这是继承映射的默认策略。 InheritanceType.JOINED：连接子类策略。父亲的放在一张表，儿子只是保存和父亲不一样的，增加的属性。 InheritanceType.TABLE_PER_CLASS： 每个具体的类一个表的策略。 @DiscriminatorColumn:在整个类层次对应一张表策略的映射策略中配置辨别列。 | 属性 | 是否必要 | 说明 | | ————– | ———— | ———— | | columnDefinition | 否 | 指定JPA使用该属性值指定的SQL片段来创建外键列 | | name | 否 | 指定辨别列的名称，默认值为”DTYPE” | | discriminatorType | 否 | 指定该辨别者列的数据类型。 DiscriminatorType.CHAR: 辨别者列的类型是字符类型，即该列只接受单个字符；DiscriminatorType.INTEGER：辨别者列的类型是整数类型，即该列只接受整数值；DiscriminatorType.STRING：辨别者列的类型是字符串类型，即该列只接受字符串值，为默认值| | length | 否 | 该属性指定辨别者的字符长度 | 2. JPA生命周期注解 @PerPersist：保存实体之前回调它修饰的方法。 @PostPersist：保存实体之后回调它修饰的方法。 @PreRemove：删除实体之前回调它修饰的方法。 @PostRemove：删除实体之后回调它修饰的方法。 @PreUpdate：更新实体之前回调它修饰的方法。 @PostUpdate：更新实体之后回调它修饰的方法。 @PostLoad：记载实体之后回调它修饰的方法。 @EntityListeners: 自定义专门的监听器 123@Entity@EntityListeners(PersonListener.class)public class Person implements Serializable&#123;&#125; @ExcludeDefaultListeners和@ExcludeSuperclassListeners：排除监听器。 3. 关联 单向N-1关联：使用@ManyToOne注解。比如一个人对应多个手机号,仅通过手机号获取用户，无需获取用户的手机号的场景。当使用@JoinColumn通过外键实现，否则通过第三方表实现。1234567891011121314151617181920212223242526272829303132333435363738@Entity(name = &quot;Person&quot;)public static class Person &#123; @Id @GeneratedValue private Long id; public Person() &#123; &#125;&#125;@Entity(name = &quot;Phone&quot;)public static class Phone &#123; @Id @GeneratedValue private Long id; @Column(name = &quot;`number`&quot;) private String number; @ManyToOne(optional=false,cascade=CascadeType.ALL,fetch=FetchType.LAZY,targetEntity=Person.class) @JoinColumn(name = &quot;person_id&quot;, foreignKey = @ForeignKey(name = &quot;PERSON_ID_FK&quot;) ) private Person person; public Phone() &#123; &#125; public Phone(String number) &#123; this.number = number; &#125; public Long getId() &#123; return id; &#125; public String getNumber() &#123; return number; &#125; public Person getPerson() &#123; return person; &#125; public void setPerson(Person person) &#123; this.person = person; &#125;&#125; 对应的sql语句：12345678910111213CREATE TABLE Person ( id BIGINT NOT NULL , PRIMARY KEY ( id ))CREATE TABLE Phone ( id BIGINT NOT NULL , number VARCHAR(255) , person_id BIGINT , PRIMARY KEY ( id ) )ALTER TABLE PhoneADD CONSTRAINT PERSON_ID_FKFOREIGN KEY (person_id) REFERENCES Person 单向1-1关联: 使用@OneToOne注解。比如一个人对应一个身份证Id,只需获取一个人的身份证号，而无需通过身份证号获取用户的情况。当使用@JoinColumn通过外键实现，否则通过第三方表实现。 1234567891011121314151617@Entity@Table(name=&quot;person_table&quot;)public class Person&#123;@Idprivate int personid;private String name;@OneToOne(optional=false,cascade=CascadeType.ALL,fetch=FetchType.LAZY,targetEntity=IdCard.class)@JoinColumn(name=&quot;id_card_id&quot;,nullable=false,updatable=false)//映射外键列private IdCard idCard;&#125;@Entity@Table(name=&quot;id_card_table&quot;)public class IdCard&#123;@Idprivate int idCardId;private String cardNumber;&#125; 单向1-N关联：使用@OneToMany注解。-对于1-N关联，应尽量设计为双向关联，而不是单向比如一个人有多个手机号，仅需要获取一个人的手机号，而无需通过手机号获取用户的场景。当使用@JoinColumn通过外键实现，否则通过第三方表实现。 1234567891011121314151617181920212223242526272829303132@Entity(name = &quot;Person&quot;)public static class Person &#123; @Id @GeneratedValue private Long id; @OneToMany(cascade = CascadeType.ALL, orphanRemoval = true) private List&lt;Phone&gt; phones = new ArrayList&lt;&gt;(); public Person() &#123; &#125; public List&lt;Phone&gt; getPhones() &#123; return phones; &#125;&#125;@Entity(name = &quot;Phone&quot;)public static class Phone &#123; @Id @GeneratedValue private Long id; @Column(name = &quot;`number`&quot;) private String number; public Phone() &#123; &#125; public Phone(String number) &#123; this.number = number; &#125; public Long getId() &#123; return id; &#125; public String getNumber() &#123; return number; &#125;&#125; 对应的sql：12345678910111213141516171819202122CREATE TABLE Person ( id BIGINT NOT NULL , PRIMARY KEY ( id ))CREATE TABLE Person_Phone ( Person_id BIGINT NOT NULL , phones_id BIGINT NOT NULL)CREATE TABLE Phone ( id BIGINT NOT NULL , number VARCHAR(255) , PRIMARY KEY ( id ))ALTER TABLE Person_PhoneADD CONSTRAINT UK_9uhc5itwc9h5gcng944pcaslfUNIQUE (phones_id);ALTER TABLE Person_PhoneADD CONSTRAINT FKr38us2n8g5p9rj0b494sd3391FOREIGN KEY (phones_id) REFERENCES Phone;ALTER TABLE Person_PhoneADD CONSTRAINT FK2ex4e4p7w1cj310kg2woisjl2FOREIGN KEY (Person_id) REFERENCES Person 单向N-N关联：使用@ManyToMany注解。比如一个人有多个住址，一个住址又对应多个用户，仅需通过用户获取住址列表的场景。对于多对多关系，数据库底层只能通过关联表实现。方式一：使用默认12345678910111213141516171819202122232425262728293031323334353637@Entity(name = &quot;Person&quot;)public static class Person &#123; @Id @GeneratedValue private Long id; @ManyToMany(cascade = &#123;CascadeType.PERSIST, CascadeType.MERGE&#125;) private List&lt;Address&gt; addresses = new ArrayList&lt;&gt;(); public Person() &#123; &#125; public List&lt;Address&gt; getAddresses() &#123; return addresses; &#125;&#125;@Entity(name = &quot;Address&quot;)public static class Address &#123; @Id @GeneratedValue private Long id; private String street; @Column(name = &quot;`number`&quot;) private String number; public Address() &#123; &#125; public Address(String street, String number) &#123; this.street = street; this.number = number; &#125; public Long getId() &#123; return id; &#125; public String getStreet() &#123; return street; &#125; public String getNumber() &#123; return number; &#125;&#125; 对应的sql：1234567891011121314151617181920CREATE TABLE Address ( id BIGINT NOT NULL , number VARCHAR(255) , street VARCHAR(255) , PRIMARY KEY ( id ))CREATE TABLE Person ( id BIGINT NOT NULL , PRIMARY KEY ( id ))CREATE TABLE Person_Address ( Person_id BIGINT NOT NULL , addresses_id BIGINT NOT NULL)ALTER TABLE Person_AddressADD CONSTRAINT FKm7j0bnabh2yr0pe99il1d066uFOREIGN KEY (addresses_id) REFERENCES Address;ALTER TABLE Person_AddressADD CONSTRAINT FKba7rc9qe2vh44u93u0p2auwtiFOREIGN KEY (Person_id) REFERENCES Person 方式二：通过@JoinTable配置关联表123456789101112131415161718@Entity(name = &quot;Person&quot;)public static class Person &#123; @Id @GeneratedValue private Long id; @ManyToMany(cascade = &#123;CascadeType.PERSIST, CascadeType.MERGE&#125;,targetEntity=Address.class) @JoinTable( name=&quot;person_address&quot;, joinColumns=@JoinColumn(name=&quot;person_id&quot;), inverseJoinTableColumns=@JoinColumn(name=&quot;address_id&quot;) ) private List&lt;Address&gt; addresses = new ArrayList&lt;&gt;(); public Person() &#123; &#125; public List&lt;Address&gt; getAddresses() &#123; return addresses; &#125;&#125; 双向1-1关联: 使用两边@OneToOne注解和mappedBy属性双向需要两边实体类都增加@OneToOne，可在一边的实体类增加mappedBy属性。当使用mappedBy属性后表示当前实体不再控制关联联系，因此不可使用@JoinColumn。比如一个人一个精确住址，既可以通过用户获取住址，又可以通过住址获取该住户的场景。 1234567891011121314151617181920212223@Entity@Table(name=&quot;person_table&quot;)public class Person&#123;@Id@GeneratedValue(strategy=GenerationType.IDENTITY)private int personId;private String name;private int age;@OneToOne(mappedBy=&quot;person&quot;,cascade=CascadeType.ALL)private Address address;//...&#125;@Entity@Table(name=&quot;address_table&quot;)public class Address&#123; @Id private int addressId; private String detail; @OneToOne(optional=false,cascade=CascadeType.ALL) @JoinColumn(name=&quot;person_id&quot;,nullable=false,updatable=false) private Person person; //...&#125; 双向1-N关联：使用@OneToMany和@ManyToOne注解和mappedBy属性对于1-N关联，应尽量设计为双向关联，而不是单向，并且尽量使用N的一端来控制关联。1的一端使用@OneToMany注解和mappedBy属性，N的一端使用@ManyToOne和@JoinColumn。比如一个人有多个住址，既可以通过用户获取住址，又可以通过住址获取用户的场景。 1234567891011121314151617181920212223@Entity@Table(name=&quot;person_table&quot;)public class Person&#123;@Id@GeneratedValue(strategy=GenerationType.IDENTITY)private int personId;private String name;private int age;@OneToMany(mappedBy=&quot;person&quot;,cascade=CascadeType.ALL)private Set&lt;Address&gt; addresses=new HashSet&lt;Address&gt;();//...&#125;@Entity@Table(name=&quot;address_table&quot;)public class Address&#123; @Id private int addressId; private String detail; @ManyToOne(optional=false,cascade=CascadeType.ALL) @JoinColumn(name=&quot;person_id&quot;,nullable=true) private Person person; //...&#125; 双向N-N关联: 使用两边@ManyToMany注解，一边mapperBy属性和。对于N-N关联，底层数据库必须通过关联表来关联实体之间的关系。对于双向N-N关联，两边实体对等，一边通过mappedBy不再控制关系，另一边通过@JoinTable控制关系即可。比如多个人住在同一个地址，但一个人也可有多个住址，既可以通过用户找到住址列表，又可以通过住址找到用户列表的场景。 12345678910111213141516171819202122232425262728@Entity@Table(name=&quot;person_table&quot;)public class Person&#123;@Id@GeneratedValue(strategy=GenerationType.IDENTITY)private int personId;private String name;private int age;@ManyToMany(mappedBy=&quot;persons&quot;,cascade=CascadeType.ALL)private Set&lt;Address&gt; addresses=new HashSet&lt;Address&gt;();//...&#125;@Entity@Table(name=&quot;address_table&quot;)public class Address&#123; @Id private int addressId; private String detail; @ManyToMany(optional=false,cascade=CascadeType.ALL) @JoinColumn(name=&quot;person_id&quot;,nullable=true) @JoinTable( name=&quot;person_address&quot;, joinColumns=@JoinColumn(name=&quot;address_id&quot;), inverseJoinTableColumns=@JoinColumn(name=&quot;person_id&quot;) ) private Set&lt;Person&gt; persons=new HashSet&lt;Person&gt;(); //...&#125; 使用Map集合记录关联实体：使用@MapKey注解：比如一个人有多个住址，既可以通过用户获取住址，又可以通过住址获取用户的场景。使用@MapKey时必须指定一个name属性，name属性的属性值为当前实体的关联实体中标识属性的属性名。 12345678910111213@Entity@Table(name=&quot;person_table&quot;)public class Person&#123;@Id@GeneratedValue(strategy=GenerationType.IDENTITY)private int personId;private String name;private int age;@OneToMany(mappedBy=&quot;person&quot;,cascade=CascadeType.ALL)@MapKey(name=&quot;pk&quot;)private Map&lt;AddressPk,Address&gt; addresses=new HashMap&lt;AddressPk,Address&gt;();//...&#125; 4. JPA映射策略1234567891011121314JPA提供了3种映射策略：(1)、 整个类层次对应一张表策略,这是继承映射的默认策略。即如果实体类B继承实体类A，实体类C也继承自实体A，那么只会映射成一个表，这个表中包括了实体类A、B、C中所有的字段，JPA使用一个叫做“discriminator列”来区分某一行数据是应该映射成哪个实体。注解为：@Inheritance(strategy = InheritanceType.SINGLE_TABLE)(2)、 连接子类策略。父亲的放在一张表，儿子只是保存和父亲不一样的，增加的属性。这种情况下子类的字段被映射到各自的表中，这些字段包括父类中的字段，并执行一个join操作来实例化子类。注解为：@Inheritance(strategy = InheritanceType.JOINED)(3)、 每个具体的类一个表的策略。注解为：@Inheritance(strategy = InheritanceType.TABLE_PER_CLASS)可使用@Inheritance指定映射策略InheritanceType.SINGLE_TABLE：第一种InheritanceType.JOINED：第二种InheritanceType.TABLE_PER_CLASS：第三种 整个类层次对应一张表策略。这种策略下，整个类层次所有的实体都存放在一张数据表中，系统通过在该表增加额外的一个辨别列，用来区分每行记录到底是哪一个类的实例。使用@DiscriminatorColumn来配置辨别列。 123456789101112131415161718192021@Entity@Inheritance(strategy=InheritanceType.SINGLE_TABLE)// 定义辨别者列的列名为person_type，列类型为字符串@DiscriminatorColumn(name=&quot;person_type&quot; , discriminatorType=DiscriminatorType.STRING)// 指定Person实体对应的记录在辨别者列的值为&quot;普通人&quot;@DiscriminatorValue(&quot;普通人&quot;)@Table(name=&quot;person_inf&quot;)public class Person&#123;&#125;// 顾客类继承了Person类@Entity// 指定Customer实体对应的记录在辨别者列的值为&quot;顾客&quot;@DiscriminatorValue(&quot;顾客&quot;)@Table(name=&quot;customer_inf&quot;)public class Customer extends Person&#123;&#125;// 员工类继承了Person类@Entity// 指定Employee实体对应的记录在辨别者列的值为&quot;员工&quot;@DiscriminatorValue(&quot;员工&quot;)@Table(name=&quot;employee_inf&quot;)public class Employee extends Person&#123;&#125; 连接子类的映射策略这种策略中父类实体保存在父类表中，而子类实体由父亲表和子类表共同存储，父类和子类共有部分存储在父类表，子类单独存在属性存储在子类表中。无需使用辨别者，只需要在继承树的根实体类上使用@Inheritance,指定strategy=InheritanceType.JOINED即可 12345678910@Entity@Inheritance(strategy=InheritanceType.JOINED)@Table(name=&quot;person_inf&quot;)public class Person&#123;&#125;@Entity@Table(name=&quot;customer_inf&quot;)public class Customer extends Person&#123;&#125;@Entity@Table(name=&quot;employee_inf&quot;)public class Employee extends Person&#123;&#125; 每个具体的类一个表的策略子类实例仅保存在子类表中，在父类表中没有任何记录。单从数据库来看，几乎难以看出继承关系，只是多个实体之间主键存在某种连续性，因此不能让数据库自动生成主键，因此不能使用GenerationType.IDENTITY和GenerationType.AUTO这两种主键生成策略。无需使用辨别者，只需要在继承树的根实体类上使用@Inheritance,指定strategy=InheritanceType.TABLE_PER_CLASS即可 1234@Entity@Inheritance(strategy=InheritanceType.TABLE_PER_CLASS)@Table(name=&quot;person_inf&quot;)public class Person&#123;&#125; 5. spring data注解 @CreatedBy：Declares a field as the one representing the principal that created the entity containing the field. @CreatedDate：Declares a field as the one representing the date the entity containing the field was created. @Id：Demarcates an identifier. @LastModifiedBy：Declares a field as the one representing the principal that recently modified the entity containing the field. @LastModifiedDate：Declares a field as the one representing the date the entity containing the field was recently modified. @ReadOnlyProperty：Marks a field to be read-only for the mapping framework and therefore will not be persisted. @Reference：Meta-annotation to be used to annotate annotations that mark references to other objects. @Transient：Marks a field to be transient for the mapping framework. Thus the property will not be persisted and not further inspected by the mapping framework. @TypeAlias：Annotation to allow String based type aliases to be used when writing type information for PersistentEntitys. @version: 定义一个属性为版本字段用于实现乐观锁 参考 《经典JAVAEE企业应用实战》 JPA API spring data commons源码 http://blog.csdn.net/u012881904/article/details/51059156","tags":[{"name":"java,JPA","slug":"java-JPA","permalink":"http://yoursite.com/tags/java-JPA/"}]},{"title":"GitLearn in 15 minutes","date":"2017-03-22T05:44:44.324Z","path":"2017/03/22/GitLearn in 15 minutes/","text":"学习来源：https://try.github.io/levels/1/challenges/1Git是一个开源的分布式版本控制系统（DVCS），可以显着改善您在项目中的工作方式和协作。 Git允许您保留对本地文件进行重大更改的历史记录。它还可以通过像GitHub这样的远程托管服务来备份这个历史记录。1.1 Got 15 minutes and want to learn Git?Git allows groups of people to work on the same documents (often code) at the same time, and without stepping on each other’s toes. It’s a distributed version control system. Our terminal prompt below is currently in a directory we decided to name “octobox”. To initialize a Git repository here, type the following command: Git允许一群人同时处理相同的文档（通常是代码），而不会踩在彼此的脚趾上。它是一个分布式版本控制系统。 我们的终端提示如下，目前我们决定命名为“octobox”。要在此处初始化Git存储库，请键入以下命令： 1git init 现在就创建了一个Git储存仓库并且生成了 .git文件夹 这个文件夹是用来存放有关仓库的所有的信息 1.2 Checking the Status （检查状态） Good job! As Git just told us, our “octobox” directory now has an empty repository in /.git/. The repository is a hidden directory where Git operates. To save your progress as you go through this tutorial – and earn a badge when you successfully complete it – head over to create a free Code School account. We’ll wait for you here. Next up, let’s type the git status command to see what the current state of our project is: 做得好！正如Git刚刚告诉我们的，我们的“octobox”目录现在在/.git/中有一个空的存储库。存储库是Git操作的隐藏目录。 通过本教程来节省您的进度，并在成功完成课程时获得徽章，以创建免费的Code School帐户。我们会在这里等你 接下来，我们输入git status命令来查看我们项目的当前状态： 1git status 经常运行git status是很健康的。有时事情改变，你不注意它。 可以看到当前的状态是 master 也就是主分支的状态 初始化commit 检查到没有东西要提交 因为仓库中是空的现在 1.3 Adding &amp; Committing I created a file called octocat.txt in the octobox repository for you (as you can see in the browser below). You should run the git status command again to see how the repository status has changed: 我在octobox存储库中为您创建了一个名为octocat.txt的文件（您可以在下面的浏览器中看到）。 您应该再次运行git status命令以查看存储库状态如何更改： staged: Files are ready to be committed. 这时候是用红色字体来显示的 说明文件已准备好提交。 unstaged: Files with changes that have not been prepared to becommitted. 在检查的时候发现一个具有尚未准备提交的更改的文件 Files aren’t tracked by Git yet. This usually indicates a newlycreated file. deleted: Git尚未跟踪文件。这通常表示新创建的文件。 deleted: File has been deleted and is waiting to be removed from Git.文件已被删除，正在等待从Git中删除 1.4 Adding ChangesGood, it looks like our Git repository is working properly. Notice how Git says octocat.txt is “untracked”? That means Git sees that octocat.txt is a new file. To tell Git to start tracking changes made to octocat.txt, we first need to add it to the staging area by using git add. 好的，看起来我们的Git仓库正常工作。注意Git如何说octocat.txt是“未被追踪的”？这意味着Git看到octocat.txt是一个新的文件。 要告诉Git开始跟踪对octocat.txt进行的更改，我们首先需要使用git add将其添加到暂存区域。 add all: You can also type git add -A . where the dot stands for thecurrent directory, so everything in and beneath it is added. The -Aensures even file deletions are included. 您也可以输入git add -A。代表当前目录下的所有文件都被添加到暂存仓库中。 -A 确保包括文件删除。 git reset: You can use git reset &lt;filename&gt; to remove a file or filesfrom the staging area 您可以使用git reset 从分段区域中删除文件。 1.5 Checking for Changes再次检查状态 Staging Area: A place where we can group files together before we“commit” them to Git. Commit 暂存区域：在我们将“提交”到Git之前，我们可以将文件暂存在一起的地方。 A “commit” is a snapshot of our repository. This way if we ever needto look back at the changes we’ve made (or if someone else does), wewill see a nice timeline of all changes. 一次“提交”是我们存储库的一个快照。这样，如果我们需要回顾我们所做的更改（或者如果有人做的），我们将会看到一个很好的时间轴关于这些的改变。 1.6 CommittingNotice how Git says changes to be committed? The files listed here are in the Staging Area, and they are not in our repository yet. We could add or remove files from the stage before we store them in the repository. To store our staged changes we run the commit command with a message describing what we’ve changed. Let’s do that now by typing: 注意Git如何说改变被提交了？这里列出的文件位于“暂存区”中，它们不在我们的存储库中。我们可以在将它们存储在存储库之前，从暂存区添加或删除文件。 要存储我们改变了什么，我们运行commit命令，并附带一条描述我们已经更改的消息。现在我们来输入： 1$ git commit -m \"Add cute octocat story\" Wildcards: We need quotes so that Git will receive the wildcard before our shell can interfere with it.Without quotes our shell will only execute the wildcard search within the current directory.Git will receive the list of files the shell found instead of the wildcard andit will not be able to add the files inside of the octofamily directory. 通配符： 我们需要引号，以便在我们的shell可以干扰之前，Git会收到通配符。没有引号，我们的shell只会在当前目录中执行通配符搜索。 Git将收到shell找到的文件列表，而不是通配符，它​​将无法在octofamily目录中添加文件。 1.7 Adding All Changes Great! You also can use wildcards if you want to add many files of the same type. Notice that I’ve added a bunch of .txt files into your directory below. I put some in a directory named “octofamily” and some others ended up in the root of our “octobox” directory. Luckily, we can add all the new files using a wildcard with git add. Don’t forget the quotes! 棒极了，如果要添加相同类型的许多文件，也可以使用通配符。请注意，我已经将一堆.txt文件添加到您的目录下面。 一个名为“octofamily”的目录中有两个，其他一些最终在我们的“octobox”目录的根。幸运的是，我们可以使用git add通配符添加所有新文件。不要忘记引号！ Check all the things!When using wildcards you want to be extra careful when doing commits. Make sure to check what files and folders &gt;are staged by using git status before you do the actual commit. This way you can be sure you’re committing only the things you want. 检查所有的事情！ 当使用通配符时，你在做提交时要特别小心。确保在执行实际提交之前使用git status检查要暂存的文件和文件夹。这样，你可以确保你t提交的只是你想要的东西。 1.8 Committing All Changes Okay, you’ve added all the text files to the staging area. Feel free to run git status to see what you’re about to commit. If it looks good, go ahead and run: More useful logs:Use git log –summary to see more information foreach commit. You can see where new files were added for the first timeor where files were deleted. It’s a good overview of what’s going on in the project. 更有用的日志： 使用git log –summary查看每个提交的更多信息。您可以看到第一次添加新文件的位置或删除文件的位置。这是一个很好的概述，在项目中发生了什么。 1.9 HistorySo we’ve made a few commits. Now let’s browse them to see what we changed. Fortunately for us, there’s git log. Think of Git’s log as a journal that remembers all the changes we’ve committed so far, in the order we committed them. Try running it now: 所以我们做了一些提交。现在让我们浏览他们看看我们改变了什么。 幸运的是，我们有git log。想想Git的日志是一本记录，记录我们迄今为止所做的所有变化，按照我们提交的顺序。尝试运行它： git remote: Git doesn’t care what you name your remotes, but it’s typical to name your main one origin. It’s also a good idea for your main repository to be on a remote server like GitHub in case yourmachine is lost at sea during a transatlantic boat cruise or crushedby three monkey statues during an earthquake. git远程 Git不在乎你命名你的远程，但它的典型名称是你的主要来源（orign）。 如果您的主机位于远程服务器，如GitHub，如果您的机器在海洋中丢失，在跨大西洋的船只巡航或在地震期间被三只猴子雕像压碎，这也是一个好主意。 1.10 Remote Repositories（远程仓库）Great job! We’ve gone ahead and created a new empty GitHub repository for you to use with Try Git at https://github.com/try-git/try_git.git. To push our local repo to the GitHub server we’ll need to add a remote repository. This command takes a remote name and a repository URL, which in your case is https://github.com/try-git/try_git.git. Go ahead and run git remote add with the options below: 做得好！我们已经开始创建了一个新的空的GitHub存储库，供您与Try Git一起使用，网址为https：//github.com/try-git/try_git.git。要将我们的本地repo推送到GitHub服务器，我们需要添加一个远程仓库。 此命令使用远程名称和存储库URL，在您的情况下，它是https://github.com/try-git/try_git.git。 继续运行git远程添加与以下选项： 1git remote add origin https://github.com/try-git/try_git.git Cool Stuff:When you start to get the hang of git you can do some really cool things with hooks when you push.For example, you can upload directly to a webserver whenever you push to your master remoteinstead of having to upload your site with an ftp client. Check outCustomizing Git - Git Hooks for more information. 很酷的东西： 当你开始得到git的挂起，你可以做一些非常酷的东西与钩子，当你push。 例如，您可以直接上传到网络服务器，只要您推送到主控远程，而不必使用ftp客户端上传您的站点。查看自定义Git - Git Hooks了解更多信息。 1.11 Pushing RemotelyThe push command tells Git where to put our commits when we’re ready, and now we’re ready. So let’s push our local changes to our origin repo (on GitHub). The name of our remote is origin and the default local branch name is master. The -u tells Git to remember the parameters, so that next time we can simply run git push and Git will know what to do. Go ahead and push it! push命令告诉Git当我们准备好时，在哪里放置我们的提交，现在我们准备好了。所以我们把我们的地方变化推到我们的起始地位（在GitHub上）。 我们的远程的名称是origin，默认的本地分支名称是master。 -u告诉Git记住参数，所以下次我们可以简单的运行git push，Git会知道该怎么做。继续推动！ 1$ git push -u origin master 1.12 Pulling RemotelyLet’s pretend some time has passed. We’ve invited other people to our GitHub project who have pulled your changes, made their own commits, and pushed them. We can check for changes on our GitHub repository and pull down any new changes by running: 假设有一段时间过去了我们邀请其他人加入我们的GitHub项目，他们已经拉下了你的修改，提交了自己的提交，并推送他们。 我们可以检查我们的GitHub存储库中的更改，并通过运行以下命令来删除任何新的更改 1git pull origin master HEADThe HEAD is a pointer that holds your position within all yourdifferent commits. By default HEAD points to your most recent commit,so it can be used as a quick way to reference that commit withouthaving to look up the SHA. 头 HEAD是一个指针，可以在您所有不同的提交内保持您的位置。默认情况下，HEAD指向您最近的提交，因此可以将其用作引用该提交的快速方式，而无需查找SHA 1.13 DifferencesUh oh, looks like there have been some additions and changes to the octocat family. Let’s take a look at what is different from our last commit by using the git diff command. In this case we want the diff of our most recent commit, which we can refer to using the HEAD pointer. 呃哦，看起来已经有了一些补充和改变octocat家庭。让我们通过使用git diff命令来看看与上一次提交不同的是什么。 在这种情况下，我们需要我们最近提交的diff，我们可以参考使用HEAD指针。 1git diff HEAD Commit Etiquette: You want to try to keep related changes together in separate commits. Using ‘git diff’ gives you a good overview ofchanges you have made and lets you add files or directories one at atime and commit them separately. 提交礼仪： 您想尝试将相关更改保留在单独的提交中。使用’git diff’可以很好地概述所做的更改，并允许您一次添加一个文件或目录，并单独提交它们。 1.14 Staged DifferencesAnother great use for diff is looking at changes within files that have already been staged. Remember, staged files are files we have told git that are ready to be committed. Let’s use git add to stage octofamily/octodog.txt, which I just added to the family for you. diff的另一个很好的用途是查看已经上架的文件中的更改。记住，暂存文件是我们已经告诉git的文件，可以提交。 让我们使用git add来分配octofamily / octodog.txt，我刚刚为你添加了家庭。 1git add octofamily/octodog.txt 1.15 Staged Differences (cont’d)Good, now go ahead and run git diff with the –staged option to see the changes you just staged. You should see that octodog.txt was created.好，现在继续运行git diff与–staged选项，以查看刚刚暂存的更改。你应该看到octodog.txt被创建。 1git diff --staged 1.16 Resetting the StageSo now that octodog is part of the family, octocat is all depressed. Since we love octocat more than octodog, we’ll turn his frown around by removing octodog.txt. You can unstage files by using the git reset command. Go ahead and remove octofamily/octodog.txt. 所以现在octodog是家庭的一部分，octocat是所有郁闷。因为我们喜欢octocat超过octodog，我们将转过他的皱眉，删除octodog.txt。 您可以使用git reset命令来解压缩文件。继续去除octofamily / octodog.txt。 1git reset octofamily/octodog.txt The ‘–’So you may be wondering, why do I have to use this ‘–’ thing? git checkout seems to work fine without it. It’s simply &gt;promising the command line that there are no more options after the ‘–’. This way if you happen to have a branch &gt;named octocat.txt, it will still revert the file, instead of switching to the branch of thesame name. ‘ - ‘ 所以你可能会想，为什么我必须使用这个’ - ‘的东西？ git结帐似乎工作正常没有它。它只是承诺命令行，在’ - ‘之后没有更多的选项。这样，如果碰巧有一个名为octocat.txt的分支，它仍然会还原文件，而不是切换到相同名称的分支。 1.17 Undogit reset did a great job of unstaging octodog.txt, but you’ll notice that he’s still there. He’s just not staged anymore. It would be great if we could go back to how things were before octodog came around and ruined the party. Files can be changed back to how they were at the last commit by using the command: git checkout – . Go ahead and get rid of all the changes since the last commit for octocat.txt git reset做了一个伟大的工作，不在暂存octodog.txt，但你会注意到，他还在那里。他只是没有在暂存区了。如果我们可以回到上次的时候，那将是很美好的。 可以使用以下命令将文件更改回上次提交的方式：git checkout - &lt;target&gt;。继续前进，摆脱上次提交octocat.txt以来的所有更改 1git checkout -- octocat.txt Branching Branches are what naturally happens when you want to work onmultiple features at the same time. You wouldn’t want to end up with amaster branch which has Feature A half done and Feature B half done.Rather you’d separate the code base into two “snapshots” (branches)and work on and commit to them separately. As soon as one was ready,you might merge this branch back into the master branch and push it tothe remote server. 分枝 当您想同时处理多个功能时，分支机构是自然会发生的。你不想结束一个主分支，其中功能A一半完成，功能B一半完成。 相反，你会将代码库分成两个“快照”（分支），并分别处理和提交。一旦准备就绪，您可以将该分支合并回主分支并将其推送到远程服务器。 1.18 Branching OutWhen developers are working on a feature or bug they’ll often create a copy (aka. branch) of their code they can make separate commits to. Then when they’re done they can merge this branch back into their main master branch. We want to remove all these pesky octocats, so let’s create a branch called clean_up, where we’ll do all the work: 当开发人员正在开发一个功能或错误时，他们经常会创建一个他们的代码的副本（也称为分支），他们可以单独提交。然后当他们完成后，他们可以合并这个分支回他们的主要分支。 我们想删除所有这些讨厌的octocats，所以让我们创建一个名为clean_up的分支，我们将在其中完成所有的工作： 1$ git branch clean_up You can use:git checkout -b new_branch to checkout and create abranch at the same time. This is the same thing as doing: git branchnew_branch git checkout new_branch 中文(简体)您可以使用： git checkout -b new_branch 同时创建并且创建一个分支。他们和这样是在做事情： git branch new_branch git checkout new_branch Remove all the things! Removing one file is great and all, but what if you want to remove an entire folder? You can use the recursive option on git rm: git rm -r folder_of_cats This will recursively remove allfolders and files from the given directory. 删除一个文件是伟大的，所有，但如果要删除整个文件夹怎么办？您可以在git rm上使用递归选项： git rm -r folder_of_cats 这将递归地删除给定目录中的所有文件夹和文件。 1.20 Removing All The ThingsOk, so you’re in the clean_up branch. You can finally remove all those pesky octocats by using the git rm command which will not only remove the actual files from disk, but will also stage the removal of the files for us. You’re going to want to use a wildcard again to get all the octocats in one sweep, go ahead and run: 的，所以你在clean_up分支。您可以使用git rm命令，最终删除所有那些讨厌的octocats，它不仅可以从磁盘中删除实际的文件，还可以为我们移除文件。 您将要再次使用通配符将所有octocats一次性扫描，然后运行： 1git rm '*.txt' The ‘-a’ option If you happen to delete a file without using ‘git rm’you’ll find that you still have to &#39;git rm&#39; the deleted files from the working tree. You can save this step by using the ‘-a’ option on &#39;git commit&#39;, which auto removes deleted files with the commit. git commit -am &quot;Delete stuff&quot; ‘-a’选项 如果你碰巧删除一个文件而不使用’git rm’，你会发现你仍然需要从工作树中删除已删除的文件。您可以使用’git commit’上的’-a’选项来保存此步骤，该选项会通过提交自动删除已删除的文件。 git commit -am“删除东西” 1.21 Commiting Branch ChangesNow that you’ve removed all the cats you’ll need to commit your changes. Feel free to run git status to check the changes you’re about to commit.现在你已经删除了所有需要提交更改的猫。 随意运行git状态以检查您要提交的更改。 1.22 Switching Back to masterGreat, you’re almost finished with the cat… er the bug fix, you just need to switch back to the master branch so you can copy (or merge) your changes from the clean_up branch back into the master branch. Go ahead and checkout the master branch:好的，你几乎完成了这个错误修复，你只需要切换回主分支，所以你可以将你的更改从clean_up分支复制（或合并）到主分支中。 继续检查主分支： 1git checkout master Pull RequestsIf you’re hosting your repo on GitHub, you can do something called a pull request.A pull request allows the boss of the project to look through your changes and make comments before deciding to merge in the change. It’s a really great feature that is used all the time for remote workers and open-source projects.Check out the pull request help page for more information. 拉取请求 如果您在GitHub上托管您的项目，您可以进行一些称为拉动请求的操作。 拉动请求允许项目的老板在决定合并更改之前，先查看更改并发表评论。这是一个非常好的功能，它一直用于远程工作和开源项目。 请查看拉动请求帮助页面以获取更多信息。 1.23 Preparing to MergeAlrighty, the moment has come when you have to merge your changes from the clean_up branch into the master branch. Take a deep breath, it’s not that scary. We’re already on the master branch, so we just need to tell Git to merge the clean_up branch into it: 好的，当你必须将clean_up分支中的更改合并到主分支时，现在已经到了。深吸一口气，这不是那么可怕。 我们已经在主分支上，所以我们只需要告诉Git将clean_up分支合并到它中： 1git merge clean_up Merge ConflictsMerge Conflicts can occur when changes are made to afile at the same time. A lot of people get really scared when aconflict happens, but fear not! They aren’t that scary, you just needto decide which code to keep. Merge conflicts are beyond the scope ofthis course, but if you’re interested in reading more, take a look thesection of the Pro Git book on how conflicts are presented. 合并冲突 合并冲突可能会发生在同时对文件进行更改时。很多人在发生冲突时真的害怕，但不要害怕！他们不是那么可怕，你只需要决定保留哪些代码。 合并冲突超出了本课程的范围，但如果您有兴趣阅读更多内容，请查看Pro Git书中有关如何呈现冲突的部分 Force deleteWhat if you have been working on a feature branch and youdecide you really don’t want this feature anymore? You might decide todelete the branch since you’re scrapping the idea. You’ll notice thatgit branch -d bad_feature doesn’t work. This is because -d won’t letyou delete something that hasn’t been merged. You can either add the--force (-f) option or use -D which combines -d -f together into one command. 强制删除 如果您在功能部门工作，而且您决定不再需要此功能，该怎么办？你可能会决定删除分支，你会注意到git branch -d bad_feature不工作。这是因为-d不会让您删除未合并的内容。 您可以添加–force（-f）选项，或者使用-D组合-d -f到一个命令中。 1.24 Keeping Things CleanCongratulations! You just accomplished your first successful bugfix and merge. All that’s left to do is clean up after yourself. Since you’re done with the clean_up branch you don’t need it anymore. You can use git branch -d &lt;branch name&gt; to delete a branch. Go ahead and delete the clean_up branch now: 恭喜！你刚刚完成了你的第一个成功的修正和合并。所有剩下要做的就是清理自己。因为你已经完成了clean_up分支，你不再需要它。 你可以使用git branch -d &lt;branch name&gt;删除一个分支。继续并删除clean_up分支： 1.25 The Final PushHere we are, at the last step. I’m proud that you’ve made it this far, and it’s been great learning Git with you. All that’s left for you to do now is to push everything you’ve been working on to your remote repository, and you’re done! 在这里，我们在最后一步。我很自豪，你已经做到这一点，这是伟大的学习Git与你。现在剩下的一切就是把你一直在做的一切都推送到你的远程仓库，你已经完成了！ 1git push Learning more about GitWe only scratched the surface of Git in this course. There is so much more you can do with it. Check out the Git documentation for a full list of functions.The Pro Git book, by Scott Chacon, is an excellent resource to teach you the inner workings of Git.help.github and GitHub Training are also great for anything related to Git in general and using Git with GitHub. 学习更多关于Git 我们在这个课程中只刮了Git的表面。还有更多的你可以做到这一点。查看Git文档的完整功能列表。 由Scott Chacon撰写的Pro Git书是教你Git内部工作的绝佳资源。 help.github和GitHub培训对于与Git有关的任何事情也是非常好的，并且使用Git与GitHub。","tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}]},{"title":"通过Git上传项目的Github","date":"2017-03-22T03:15:43.017Z","path":"2017/03/22/通过Git上传项目的Github/","text":"通过Git上传项目的Github首先我们先来介绍一下GitGit 与版本控制 版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。 虽然基于 Git 的工作流可能并不是一个非常好的实践，但是在这里我们以这个工作流做为实践来开展我们的项目。如下图所示是一个基于 Git 的项目流： 我们日常会工作在 “develop” 分支（那条线）上，通常来说每个迭代我们会发布一个新的版本，而这个新的版本将会直接上线到产品环境。那么上线到产品环境的这个版本就需要打一个版本号——这样不仅可以方便跟踪我们的系统，而且当出错的时候我们也可以直接回滚到上一个版本。如果在上线的时候有些 Bug 不得不去修复，并且由于上线的新功能很重要，我们就需要 修补程序。而从整个过程来看，版本控制起了一个非常大的作用。 不仅仅如此，版本控制的最大重要是在开发的过程中扮演的角色。通过版本管理系统，我们可以： 将某个文件回溯到之前的状态。 将项目回退到过去某个时间点。 在修改 Bug 时，可以查看修改历史，查出修改原因 只要版本控制系统还在，你可以任意修改项目中的文件，并且还可以轻松恢复。 常用的版本管理系统有 Git、SVN，但是从近年来看 Git 似乎更受市场欢迎。 ##Git 从一般开发者的角度来看，Git 有以下功能： 从服务器上克隆数据库（包括代码和版本信息）到单机上。 在自己的机器上创建分支，修改代码。 在单机上自己创建的分支上提交代码。 在单机上合并分支。 新建一个分支，把服务器上最新版的代码 fetch 下来，然后跟自己的主分支合并。 -生成补丁（patch），把补丁发送给主开发者。 看主开发者的反馈，如果主开发者发现两个一般开发者之间有冲突（他们之间可以合作解决的冲突），就会要求他们先解决冲突，然后再由其中一个人提交。如果主开发者可以自己解决，或者没有冲突，就通过。 一般开发者之间解决冲突的方法，开发者之间可以使用 pull 命令解决冲突，解决完冲突之后再向主开发者提交补丁。从主开发者的角度（假设主开发者不用开发代码）看，Git 有以下功能：1.查看邮件或者通过其它方式查看一般开发者的提交状态。2.打上补丁，解决冲突（可以自己解决，也可以要求开发者之间解决以后再重新提交，如果是开源项目，还要决定哪些补丁有用，哪些不用）。3.向公共服务器提交结果，然后通知所有开发人员。 Git 初入如果是第一次使用 Git，你需要设置署名和邮箱： 12$ git config --global user.name \"用户名\"$ git config --global user.email \"电子邮箱\" 你可以在 GitHub 新建免费的公开仓库或在 Coding.net 新建免费的私有仓库。 新建一个Repository 按照 GitHub 的文档 或 Coding.net 的文档 配置 SSH Key，然后将代码仓库 clone 到本地，其实就是将代码复制到你的机器里，并交由 Git 来管理： 12$ git clone git@github.com:username/repository.git或$ git clone git@git.coding.net:username/repository.git 或使用 HTTPS 地址进行 clone： 123$ git clone https://username:password@github.com/username/repository.git或$ git clone https://username:password@git.coding.net/username/repository.git 复制地址： 克隆到本地： 克隆完成后的样子，会在文件夹下生成.git这就说明已经和远程仓库建立了连接 这里的代码我后来加进去的 如果项目是空的 只有.git 你可以修改复制到本地的代码了（ symfony-docs-chs 项目里都是 rst 格式的文档）。当你觉得完成了一定的工作量，想做个阶段性的提交： 向这个本地的代码仓库添加当前目录的所有改动： 1$ git add . 或者只是添加某个文件： 1$ git add -p 我在项目中添加了了一个文件用 ==ｇｉｔ ｓｔａｔｕｓ== 来查看状态 红色的说明还没有被添加到ｇｉｔ仓库中 添加完之后 我们再次查看状态 发现已经变绿了 我们可以输入 1$ git status 可以看到状态的变化是从红色色到绿色，即 unstage 到 add。在完成添加之后，我们就可以写入相应的提交信息——如这次修改添加了什么内容 、这次修改修复了什么问题等等。在我们的工作流程里，我们使用 Jira 这样的工具来管理我们的项目，也会在我们的 Commit Message 里写上作者的名字，如下： 1$ git commit -m \"nengneng : first commit\" 在这里的nengneng 对应于用户名，后面的提交信息也会写明这个任务是干嘛的。 由于有测试的存在，在完成提交之后，我们就需要运行相应的测试来保证我们没有破坏原来的功能。因此，我们就可以PUSH我们的代码到服务器端： 1$ git push 最后 我们去github 上传的仓库看看我们上传的项目： 大功告成","tags":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"}]},{"title":"Web开发全栈工程师2017路线图","date":"2017-03-20T02:10:00.484Z","path":"2017/03/20/Web开发全栈工程师2017路线图/","text":"100+免费资源用于学习全栈Web开发下面的列表并不意味着是排他性的，它更像是一个链接的集合，帮助我一路上（并希望可以帮助你）。如你所见，我专注于Javascript，React和Node.js.还有大量关于面试准备和申请工作的信息 我还有很多书签，所以我会更新和添加链接，因为我去。如果你有贡献，请随时提交公关！目录 开始 怎么学 什么是最有用的CS书签 程序和类 Learn HTML Learn CSS Learn Javascript Learn React.js Full Stack Tutorials Learn Node.js Learn APIs Learn Databases 学习认证 Learn Git Games &amp; Challenge Websites Free Programming Books Open Source Contribution Opportunities Am I Ready to be a Developer? Software Developer Success Stories Resume’s，Portfolio，LinkedIn，面试准备和薪资信息 Start Here Take a look at the big picture: Web Developer Roadpath Youtube video outlining what to learn (similar to above, but in video format) - Watch this if you want to become a web developer My journey to becoming a web developer from scratch without a CS degree (and what I learned from it) (Medium) What happens when you type google into your address bar? [Reddit Link] Tuts Plus - The http protocol every web developer must know Find a local Web Development related Meetup! How to Learn How to Learn. Coursera Course (Not CS Specific) - Learning how to learn Repetition, Repetition, Repetition - A great discussion on study techniques Reddit Post What is the Single most useful CS Bookmark you have? What is the single most useful CS Bookmark you have? [Reddit Link] Learn X in Y Minutes What CS Majors Should Know Google’s Technical Development Guide CSS Tricks - Complete Flexbox Guide Regex Cheat Sheet DevDocs Awesome List of Everything Programming How to Break Into the Tech Industry—a Guide to Job Hunting and Tech Interviews Programs and Classes Programs The Odin Project freeCodeCamp The Essential Web Developer Course Classes SANITIZED list of 530+ free online programming/CS courses (MOOCs) with feedback(i.e. exams/homeworks/assignments) that you can start this month (December 2016) - Needs Updating Udacity Free Web Development Courses (Google) Assorted Reddit Links (Still need to sort) Tutorial/Screencast: Let’s Learn Algorithms: An Intro to Binary Search Thoughts on Coding Boot camps The Complete Guide to Bootcamps Self Study - My programming notes. 275 Pages of Content [Updated Reddit Link] [Original Reddit Link] Javascript / Node / Angular 1 &amp; 2 / React / Elm / C# / PHP / SQL / Git Google Drive What to do after Colt Steele’s course Watch And Code RegexOne - Learn Regular Expressions Google University Github Repo Learn HTMLAnyone have any cool HTML links? Learn CSS Everything you need to know about Flexbox Seriously the best Flexbox resource I’ve found A visual guide to CSS - CSS Reference CSS Pro Tips - A collection of tips to help take your CSS skills pro. 10 principles for smooth web animations Learn Javascript Javascript Docs Courses/Tutorials Courses.AngularClass.com - Topics Include: webpack, nodejs, npm, es5, es6, esnext, &amp; rxjs, typescript [Course Link] [Reddit Post] Edabit - Learn Javascript With Interactive Challenges: Earn XP, Unlock Achievements &amp; Climb The Leaderboard [Edabit - Website Link] [Reddit Post] Free Advanced JavaScript Courses - Learn Object Oriented Programming, Call Apply, and Bind, Testing, Functional Programming, and much more Intermediate Course - Udacity - OOP JS Projects JS 30 For 30 - 30 Projects for 30 Days Learn to Code with Projects - enlight.ml 13 Weeks of Javascript (Medium) - TONS of links to JS resources Articles/Books Recursion, Recursion, Recursion (Medium) Eloquent JS - Free Javascript Ebook You don’t know JS - Free, hosted on Github Javascript Garden - learn about the quirky parts of JS JS - The Good Parts .pdf Learning js Design patterns - Reusable solutions to commonly occurring problems Asynchronous Module Definition Learn React JS Official React Docs Official React Tutorial 3rd Party Tutorials 10 React Mini Patterns Top 5 Tutorials for getting started with React 10 best ReactJS tutorials React Starter Project Search Exploring the react Ecosystem! - Article Code academy React program Great Free React books - Use this link first Medium Links: React Components, Elements, and Instances (Medium) Working with React Beginners guide to React Router (Medium) Angular JS vs React JS (Medium) Full Stack Tutorials Intro to Back End Web Development Deploying Applications with Heroku Client Server Communication Serverless Stack is a comprehensive guide to creating full-stack serverless applications. Create a note taking app from scratch using React.js, AWS Lambda, API Gateway, DynamoDB, and Cognito. Express - Using a DB with Mongoose Node JS and Databases Node JS and Authentication Express JS Database Integration MERN Stack Tutorial - Mongo, Express, React, Node MongoDB MERN Tutorial Series Full Stack MERN Tutorial - Youtube Build a URL Shortener with Node, Hapi, and Mongo How to Create a Complete Express.js + Node.js + MongoDB CRUD and REST Skeleton Building web app using react.js, express.js, node.js and mongodb - Part 1, 2 Trello tribute with Phoenix, React, Redux, PostgreSQL - 12 parts Create a character voting app using React, Node.js, MongoDB and Socket.IO Building a React Universal Blog App: A Step-by-Step Guide Building a Secure RESTful Node.js app Cool stuff other people have built: Belgian Beer Explorer with React, Bootstrap, Node.js and Postgres 90 Full Stack React Examples (some with tuts) Learn Node JS Official Node.js Docs Best Resource for learning Node.js [Reddit Link] Youtube Colt Steele’s Bootcamp Node School Medium - Why the hell would you use Node? Building a modern backend API with Node Node JS Login with Passport - Youtube 10 Best Practices for Writing Node.js REST APIs Learn APIs Where to start with learning APIs [Reddit Link] What is an API? In English Please. (Medium) Build Node.js RESTful APIs in 10 Minutes Free Intro to APIs Book/Course by Zapier Google Maps API Distance Calculator Web Services API Build and Secure a Backend API Server Learn Databases SQL vs NoSQL Intro to Relational Databases - SQL, DB-API, and More! MongoDB University - Numerous classes on learning MongoDB PostgreSQL Tutorial PostgreSQL Exercises Learn PostgreSQL (Github) Try Redis Redis Tutorial Learn Authentication Authentication &amp; Authorization: OAuth Learn about JSON Web Tokens OAuth 2 Passwordless Authentication with React and Auth0 Learn Git Official Tutorial - Learn Git in 15 Minutes Official Docs Other Tuts: Git, the simple guide Learn Git Branching - Level by Level learning Learn Git in 30 Minutes - Article Here are all the Git commands I used last week and what they do (Medium) Why to Use GIT No, I have no side projects to show you Games and Challenge Websites Games to learn Programming in an easy and fun way [Reddit Link] Flex Box Froggy Flex Box Defense Edabit Coding Game Elevator Saga - JS Scratch - Absolute Basics Hacked - Mobile App teaching through puzzles Coding Challenge Websites [Reddit Link] [Reddit Link #2] Medium - 10 most popular coding challenge sites of 2016 Code Wars Coding Game Hacker Rank (some debate about this being good/bad Project Euler (math focus) Exercism Free Programming Books O’Reilly Offering Programming eBooks for Free (Reddit) GitHub - Thousands of free programming Books on every topic Non-Technical Books to make you a better Programmer (Reddit) Open Source Contribution Opportunities Contributing to Open Source on GitHub - The official GitHub guide. How do I get skilled enough to work on open source projects? Open Source Contribution Opportunites [Reddit Discussion] Redditor Form to fill out to get notified about Open Source Opportunities Up For Grabs - Browse a list of projects with curated tasks Hacktoberfest - Open source activity held every October. Easy to participate, and you get a free t-shirt! I’m afraid if I say anything on Github people will laugh at me and I will die. Am I Ready to be a Developer? Readiness Self taught front end devs… When did you know that you were “Job ready”? People who are self-taught developers, how long did it take you to get your first job? When do you know when you’re ready to start interviewing? Software developers- what is the best advice you have for people learning CS? I want a career in programming What should you know as a web dev just out of college? Reddit Discussion - newer What CS Majors Should Know - older How I got started with Side Projects - link What are some goals a beginning Self-Taught Developer should have? Computer programmers of Reddit, what is your best advice to someone who is currently learning how to code? I began teaching myself to code a year ago. I got hired at my first job 4 months ago. Here is a breakdown of somethings I was not ready for (FYI job is remote ruby/rails dev) Software Developer Success Stories Success Stories 18 months ago I didn’t know how to code, I’m now a self-taught programmer who’s made apps for the NBA, NHL, and schools like Purdue, Notre Dame, Alabama and Clemson. I’m now releasing my software under the MIT license for anyone’s use — AMA! Last year I was unemployed and miserable. Using this sub and resources, I’ve been full time employed for a year. I did it with all free resources. I wanna share with you how I did it. (IOS) I began teaching myself to code a year ago. I got hired at my first job 4 months ago. Here is a breakdown of somethings I was not ready for (FYI job is remote ruby/rails dev) I’m 32 years old, and just started my first full-time job as a developer. One year ago my programming knowledge was basically nil. Everything I learned, I found via /r/learnprogramming, so just wanted to share my experience. From zero to software developer - Not really a success story, but a lot of redditors share how they learned. Great for beginners with no path Get The JobResume, Portfolio, LinkedIn, Interview Prep, and Salary Information How to Apply I spent 3 months applying to jobs after a coding bootcamp. Here’s what I learned. (Medium) Lessons from my Post-bootcamp Job search (Medium) How to land a six figure job in tech with no connections (Medium) Resume &amp; LinkedIn What are some of the best resuмe formats you’ve seen? Model examples for Fullstack Developer LinkedIn profiles Personal Projects Recruiters, what kind of CS projects impress? Interview Prep CS50 - Prep and Practice for Technical Interviews [YouTube] How to Break Into the Tech Industry—a Guide to Job Hunting and Tech Interviews Common Javascript Interview Questions Repo Github Repo - All questions and answers Reddit Post - Discussion, with additional questions and answers Ammon Bartram - Ask an interviewer anything: interview questions, answers, mistakes Sharing some interview tips (Silicon valley employee) Job interview questions to ask the interviewer I suck at programming interviews. When solving an interview problem, talk all the time. Hiring managers (or other seasoned developers), what qualities do you look for in your ideal candidate? Post your best interview questions Been interviewing with a lot of tech startups as a frontend dev, here are the technical questions I’ve been asked (MID-SENIOR LEVEL) 10 Interview Questions every JS Developer should know (Medium) Salary Information 12/2016 Salary Sharing Thread (&lt;2 yrs Experience) Salary Negotiations and how not to set a bunch of money on fire (Medium) 10 Rules for negotiating a job offer (Medium) How not to bomb your offer negotiation (Medium)","tags":[{"name":"Web开发","slug":"Web开发","permalink":"http://yoursite.com/tags/Web开发/"}]},{"title":"Web开发者2017路线图","date":"2017-03-20T01:38:27.548Z","path":"2017/03/20/Web开发者2017路线图/","text":"2017年Web开发者的学习路线图 下面您将找到一组图表，说明您可以采取的路径和您想要采用的技术，以便成为前端，后端或devops。我为我的一位老教授制作了这些图表，他想要与大学生分享他们的观点。 如果你认为这些都可以改进反正，请做建议。 🚀 介绍 🎨前端路线图 👽 后端路线图对于后端，我个人喜欢Node JS和PHP-7的全时间，我已经尝试了最近的Go和我非常喜欢它。除了这些，如果我要选择另一个，我会去Ruby。但这只是我个人的喜好，你可以选择任何显示的语言，你会好的。 👷 DevOps的路线图 🚦 Wrap Up如果你认为任何路线图可以改进，请打开公关与任何更新，并提交任何问题。此外，我会继续改进这一点，所以你可能想要观察/ star这个存储库重新访问。 ☑ TODO [X] Add Frontend Roadmap [X] Add Backend Roadmap [X] Add DevOps Roadmap [ ] Add relevant resources for each 👬 ContributionThe roadmaps are built using Balsamiq. Project file can be found at /ROADMAPS.bmpr, open it in balsamiq, do the necessary modifications, export the diagrams as PNG files, put them in the Readme at relevant places and create a PR. Open pull request with improvements Discuss ideas in issues Spread the word Reach out to me directly at kamranahmed.se@gmail.com or on twitter @kamranahmedse Licence Roadmap to becoming a web developer in 2017 Below you find a set of charts demonstrating the paths that you can take and the technologies that you would want to adopt in order to become a frontend, backend or a devops. I made these charts for an old professor of mine who wanted something to share with his college students to give them a perspective. If you think that these can be improved in anyway, please do suggest. 🚀 Introduction 🎨 Frontend Roadmap 👽 Backend RoadmapFor the backend, personally I would prefer Node JS and PHP-7 for the full time plus I have been experimenting lately with Go and I quite like it. Apart from these, if I have to choose another one, I would go for Ruby. However this is just my personal preference, you can choose any of the shown languages and you will be good. 👷 DevOps Roadmap 🚦 Wrap UpIf you think any of the roadmaps can be improved, please do open a PR with any updates and submit any issues. Also, I will continue to improve this, so you might want to watch/star this repository to revisit. ☑ TODO [X] Add Frontend Roadmap [X] Add Backend Roadmap [X] Add DevOps Roadmap [ ] Add relevant resources for each 👬 ContributionThe roadmaps are built using Balsamiq. Project file can be found at /ROADMAPS.bmpr, open it in balsamiq, do the necessary modifications, export the diagrams as PNG files, put them in the Readme at relevant places and create a PR. Open pull request with improvements Discuss ideas in issues Spread the word Reach out to me directly at kamranahmed.se@gmail.com or on twitter @kamranahmedse Licence","tags":[{"name":"Web开发","slug":"Web开发","permalink":"http://yoursite.com/tags/Web开发/"}]},{"title":"JVM学习笔记（三）垃圾收集器与内存分配策略","date":"2017-03-18T05:20:25.061Z","path":"2017/03/18/JVM学习笔记（三）垃圾收集器与内存分配策略/","text":"第二部分 垃圾收集器与内存分配策略Java与C++之间有一堵由内存动态分配和垃圾收集技术所围成的“高墙”，墙外面的人想进去，墙里面的人想出来。概述垃圾收集（Garbage Collection，GC）需要完成的三件事情: 1 哪些内存需要回收？ -[] 内存区域-回收条件2 什么时候回收？ -[] 多线程/安全点3 如何回收？ -[] 回收算法 当要排查各种内存溢出、内存泄漏问题时，当垃圾收集称为系统达到更高并发量的瓶颈时，我们就需要对这些”自动化”的技术实施必要的监控和调节。 1.程序计数器、虚拟机栈、本地方法栈3个区域随线程而生，随线程而灭；每一个栈帧中分配多少内存基本上在类结构确定下来的时候就已知。因此这几个区域的内存分配和回收都具有确定性，不需过多考虑回收问题，方法结束或者线程结束时，内存自然就随之回收了。2.Java堆和方法区则不一样，一个接口中的多个实现类需要的内存可能不一样，一个方法中的多个分支需要的内存也可能不一样，只有在程序处于运行期间才知道会创建哪些对象，这部分内存的分配和回收都是动态的，垃圾收集器所关注的是这部分内存！ 对象已死吗？垃圾回收器在对堆进行回收前，首要确定的事情就是这些对象之间哪些还存活着，哪些已经死去？ 引用计数算法 定义：引用计数算法（Reference Counting）:给对象添加一个引用计数器，每当一个地方引用它时，计数器值就+1；当引用失效时，计数器值就-1；任何时刻计数器为0的对象就是不可能被再使用的； 优点：实现简单，判定效率高；微软的COM技术、Python中都使用了Reference Couting算法进行内存管理 缺点：由于其很难解决对象之间相互循环引用的问题，主流Java虚拟机里面都没有选用Refrence Couting算法来管理内存； 程序计数器 程序计数器（Program Counter Register）是一块比较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器； PCR为线程私有内存； 是唯一一个在Java虚拟机规范中没有规定任何OOM情况的区域； 可达性分析算法 定义：可达性分析（Reachability Analysis）判断对象存活的基本思路：通过一系列的称为GC Roots的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain）,当一个对象到GC Roots没有任何引用链相连（即GC Roots到这个对象不可达）时，则证明此对象是不可用的； ![enter description here][1] Java语言中，可作为GC Roots对象包括： 虚拟机栈（栈帧中的本地变量表）中引用的对象； 方法区中类静态属性引用的对象； 方法区中产量引用的对象； 本地方法栈中JNI（即一般的Native方法）引用的对象 再谈引用JDk1.2之后，Java对引用概念进行了扩充，将引用分为强引用、软引用、弱引用、虚引用4种，4种强度一次逐渐减弱。 强引用（Strong Reference）是指在程序代码之中普遍存在的，类似Object obj=new Object()这类的引用，只要强引用存在，对象就不会发生GC； 软引用（Soft Reference）是用来描述一些还有用但并非必须的对象。对于软引用关联着的对象，在系统将要发生OOM异常之前，将会把这些对象列进回收范围之中进行第二次回收，如果这次回收后还没有足够的内存，才会抛出OOM异常。 弱引用（Weak Reference）是用来描述非必须对象的，强度比软引用更弱，被弱引用关联的对象只能生存到下一次GC发生之前。当垃圾回收器工作时，无论当前内存是否足够，都会回收掉只被弱引用关联的对象 虚引用（Phantom Reference）也称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。 回收方法区 在方法区中进行垃圾收集的性价比一般比较低；而在Heap中，尤其是在新生代，常规应用进行一次垃圾收集一般回收70%~95%的空间，而永久代的垃圾收集效率远低于此； 永久代的垃圾收集主要回收两部分内容：废弃常量和无用的类； 回收废弃常量与回收Java堆中的对象类似； 判定一个类是否是无用的类条件相对苛刻： 该类所有实例都已被回收，即Java堆中不存在该类的任何实例； 加载该类的ClassLoader已经被回收； 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该方法。 在大量使用反射、动态代理、CGLib等ByteCode框架、动态生成JSP以及OSGi这类自定义ClassLoader的场景都需要虚拟机具备类卸载的功能，以保证永久代不会溢出。 垃圾收集算法只介绍内存回收的方法论（算法思想及发展过程），不讨论具体算法实现。 G1收集器（Garbage-First）面向服务器端应用的垃圾收集器，计划未来替代CMS收集器。 理解GC日志垃圾收集器参数总结内存分配与回收策略对象优先在Eden分配大对象直接进入老年代长期存活的对象将进入老年代动态对象年龄判定空间分配担保","tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}]},{"title":"JVM学习笔记（二）Java内存区域与内存溢出异常","date":"2017-03-18T02:35:33.376Z","path":"2017/03/18/JVM学习笔记（二）Java内存区域与内存溢出异常/","text":"第二部分 自动内存管理机制《深入理解Java虚拟机 JVM高级特性与最佳实践》 第二章 Java内存区域与内存溢出异常运行时的数据区域 程序计数器 程序计数器（Program Counter Register）是一块比较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器； PCR为线程私有内存； 是唯一一个在Java虚拟机规范中没有规定任何OOM情况的区域； Java虚拟机栈 Java虚拟机栈（Java Virtual Machine Stacks）描述的是Java方法执行的内存模型：每个方法在在执行的同时都会创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法接口等信息。每个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈出栈的过程。 Java虚拟机栈也是线程私有，它的生命周期与线程相同。 Java内存区常分为堆内存（Heap）和栈内存（Stack）； OOM情况：（1）线程请求的栈深度&gt;虚拟机所运行的最大深度；（2）虚拟机动态扩展时无法申请到足够的内存 本地方法栈 本地方法栈（Native Method Stack）与虚拟机所发挥的作用非常相似的，他们之间的区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机所使用的Native方法服务。 HotSpot虚拟机把本地方法栈和虚拟机栈合二为一； 此区域会抛StackOverflowError 和 OutofMemoryError异常 Java堆 Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块，Java Heap是所有线程共享的一块内存区域，在VM启动时创建 所有的对象实例以及数组都要在堆上分配（不绝对：栈上分配、标量替换优化技术）； Java堆是垃圾收集器管理的主要区域，也可称做GC堆（Garbage Collected Heap） 从内存回收的角度，现代收集器基本都采用分代收集算法，Java Heap可细分为新生代和老年代，再细致可分为Eden空间、From Survivor空间、To Survivor空间等–&gt;更好回收内存。 从内存分配的角度，线程共享的Java堆中可能分出多个线程私有的分配缓存区（TLAB：Thread Local Allocation Buffer）–&gt;更快分配内存。 Java堆出于逻辑连续的内存空间中，物理上可不连续，如磁盘空间一样； Java堆在实现上可时，可以实现成固定大小的，也可以按照可扩展实现（-Xmx和-Xms控制）； OOM情况：堆中没有内存完成实例分配，堆也无法再扩展时 方法区方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 也称为永久代（Permanent Generation）但随着Java8的到来，已放弃永久代改为采用Native Memory来实现方法区的规划。 此区域回收目标主要是针对常量池的回收和对类型的卸载。 运行时常量池 运行时常量池（Runtime Constants Pool）是方法区的一部分 Class文件中除了有类的版本、字段、方法、接口等描述的信息外，还有一项信息是常量池（Constant Pool Table）,用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 直接内存直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域。 能在一些场景中显著提高性能，因为避免了在Java堆和Native堆中来回复制数据。 直接内存的分配不会受到Java堆大小的限制，但会收到本机总内存（RAM以及SWAP/分页文件）大小以及处理器寻址空间的限制。 设置Xmx等参数信息时注意不能忽略直接内存，不然会引起OOM。 HotSpot虚拟机对象的创建为新生对象分配内存的分配方式由Java堆是否规整决定，而Java堆是否规整又由所采用的垃圾回收器是否带有压缩整理功能决定。 指针碰撞（Bump the Pointer）分配方式：Serial、ParNew等带有Compact过程的收集器 空闲列表（Free List）分配方式：类CMS这种基于Mark-Sweep算法的收集器 对分配内存空间的动作进行同步处理—VM采用CAS配上失败重试的方式保证更新操作的原子性； 本地线程分配缓冲（Thread Local Allocation Buffer,TLAB）：把内存分配动作按线程划分在不同 空间中进行，即每个线程在Java堆中预先分配一小块内存，虚拟机是否启用TLAB，可由-XX:+/-UseTLAB参数设定； 对象的内存布局 对象在内存中存储的布局可以分为3块区域：对象头（Header）、实例数据（Instance Data）、和对齐填充（Padding）; 对象头包含2部分信息 Mark Word,存储对象自身的运行时数据（如哈希码、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳）；由于对象头与对象自身定义的数据存储大小无关，考虑到VM的空间效率，Mark Word被设计成非固定的数据结构以便在极小的空间内存储尽量多的信息，他会根据对象的状态复用自己的存储空间。 类型指针，即对象指向它的类元数据的指针，VM通过这个指针来确定这个对象是哪个类的实例。 实例数据是对象真正存储的有效信息，也似乎程序代码中定义的各种类型的字段内容。 对齐填充，并不必然存在，没有特别含义，仅仅起占位符的作用，8byte对齐。 对象的访问定位Java程序需要通过栈上的reference数据来操作堆上的具体对象，对象访问方法取决于VM实现而定，目前主流访问方式有使用句柄和直接指针2种： 句柄访问Java堆中划分出一块内存作为句柄池，reference中存储对象的句柄地址，句柄中包含对象实例数据与类型数据各自的具体地址信息； 直接指针访问 Java堆对象的布局中必须考虑如何放置访问类型数据的相关信息，reference中存储对象地址； 两种访问方式各有优势 使用句柄访问最大的好处是reference中存储的是稳定的句柄地址，在对象被移动（GC时移动对象是很普遍的行为）时只会改变句柄中的实例数据指针，而reference本身不需要修改； 使用直接指针访问方式的最大好处是速度更快，它节省了一次指针定位的时间开销，由于对象访问在Java中非常频繁，因此这类开销积少成多后也是一项非常可观的执行成本； HotSpot虚拟机采用指针访问方式进行对象访问，从整个软件开发范围看，各种语言和框架使用句柄来访问的情况也非常常见。 实战OOM异常Java堆溢出 Java堆用于存储对象实例，只要不断创建对象，并保证GC Roots到对象之间有可达路径来避免回收机制清除这些对象，那么当对象数量到达最大堆的容量限制后就会产生OOM。 控制参数 -Xms：堆最小值 -Xmx：堆最大值 -XX:+HeapDumpOnOutOfMemoryError：让虚拟机在出现OOM异常时Dump出当前内存堆转储快照以便事后进行分析 异常信息Java.lang.OutOfMemory + Java Heap Space 解决办法以内存映像分析工具（Eclipse Memory Analyzer）对Dump出来的堆转储快照进行分析，重点是确认内存中的对象是否是必要的，即判断是内存泄漏（Memory Leak）还是内存溢出（Memory Overflow） 如果是内存泄漏：通过工具查看泄漏对象到GC Roots的引用链，掌握泄漏对象的类型信息及引用链的信息后可较准确的定位代码位置； 如果是内存溢出：可通过检查VM的堆参数（-Xmx和-Xms），与机器物理内存对比看是否可以调大；从代码检查是否存在某些对象生命周期过长，持有状态时间过长的情况，尝试减少程序运行期的内存消耗 虚拟机栈和本地方法栈溢出控制参数HotSpot虚拟机不区分虚拟机栈和本地方法栈， -Xoss（设置本地方法栈大小）：参数设置无效; -Xss（栈容量）; 异常信息关于虚拟机栈和本地方法栈，在Java虚拟机规范中描述了两种异常： 如果线程请求的栈深度 &gt; 虚拟机所允许的最大深度，抛出StackOverFlowError异常 如果虚拟机在扩展栈时无法申请到足够的内存空间，抛出OutOfMemoryError异常 解决办法 操作系统分配给每个进程的内存是有限制的，如32位Windwos限制为2G。虚拟机提供了参数来控制Java堆和方法区这两部分内存的最大值， 虚拟机栈和本地方法栈可瓜分的剩余内存=2G（操作系统限制）-Xmx（最大堆容量）-MaxPermSize（最大方法区容量）-虚拟机进程本身耗费内存；程序计数器消耗内存很小，可以忽略。 每个线程分配到的栈容量越大，可以建立的线程数就越少，建立线程时候就越容易耗尽剩余内存。 按虚拟机默认参数，栈深度在大多数情况下达到1000~2000完全没问题，对于正常方法调用（包括递归），这个深度应该完全够用；但如果是建立过多线程导致内存溢出，在不能减少线程数或者更换X64位虚拟机的情况下，就只能通过减少最大堆和减少栈容量来换取更多的线程 方法区和运行时常量区溢出运行时常量池是方法区的一部分，因此这两个区域的溢出可放在一起进行。 控制参数 -XX:PermSize（方法区最小容量） -XX:MaxPermSize （方法区最大容量） 异常信息OutOfMemoryError 后面跟随PermGen space 说明运行时常量池属于方法区（HotSpot虚拟机中的永久代）的一部分 本机直接内存溢出控制参数 DirectMemory容量可通过-XX:MaxDirectMemorySize指定，不指定默认与-Xmx(Java堆最大值)一样。 异常信息由DirectMemory导致的内存溢出，一个明显的特征是在Heap Dump文件中不会看见明显的异常； 如果发现OOM之后Dump文件很小，而程序又直接或简介使用了NIO，可以考虑是不是这方面的原因。","tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}]},{"title":"深入理解Java虚拟机（一）","date":"2017-03-17T03:47:06.765Z","path":"2017/03/17/深入理解Java虚拟机（一）/","text":"第一部分 走进Java世界上没有完美的程序，但我们不能因此而沮丧，因为写程序本来就是一个不断追求完美的过程。1.1 概述Java不仅仅是一门变成语言，还是一个由一系列计算机软件和规范形成的技术体系，这个技术体系提供的完整的 用于软件开发和跨平台部署的支持环境，并且广泛应用于嵌入式系统，移动终端，企业服务器，大型机等各种场所。 Java能获得如此广泛的认可，除了他拥有一门结构严谨，面向对象的编程语言之外，还有许多不可忽视的优点：他摆脱了硬件平台的束缚，实现了一次编写，到处运行的理想，他提供了一个向对安全的内存管理和访问机制，避免了绝大部分的内存泄漏和指针越界问题，它实现了热点检测代码和运行时编译以及优化，这使得Java应用能随着运行使劲按的增加而获得更高的性能，它有一套完整的应用程序接口，还有无数来自商业机构和开源社区的第三方库莱帮着它实现各种各样的功能。。。。。 当我们在使用一种技术的时候，如果不在依赖书本和他人就能得到这些答案，那才算是得到了“不惑”的境界 1.2 Java技术体系从广义上讲，Clojure，JRuby，Groovy等运行在Java虚拟机上的语言以及相关的程序都属于Java中技术体系中的一部分，如果型传统意义上来说，Sun公司官方定义的Java技术体系包括以下几个组成部分： Java程序设计语言 各种硬件平台上的Java虚拟机 Class文件格式 Java的API 来自商业机构和开源社区的第三方Java类库 JDK 包含了Java程序设计语言，Java虚拟机，和Java的API，来用于支持Java开发的最小的环境，另外的，Java API中的Java SE API子集和Java虚拟机这两部分统称为JRE（Java Runtime Environment），JRE 支持Java运行的标准环境。 以上Java技术体系图是根据各个组成部分的功能来进行划分的。如果按照Java技术服务的领域来划分，可分为以下4个平台： 1.Java Caard2.Java ME3.JavaSE（Standard Edition）：支持面向桌面级应用的java平台，提供了完整的Java核心API，这个版本以前称为J2SE。 JavaEE（Enterprise Edition）：支持使用多层架构的企业应用（如ERP、CRM应用)的Java平台，除了提高Java SE API外，还对其做了打了扩充，并提供了相关部署支持，这个版本以前称为J2EE。javaEE对JavaSE提供的扩展一般以java.作为包名，而以java.为包名的都是javaSE API的核心包，但由于历史原因，一部分曾经是扩展包的API后来进入了核心包，因此核心包中也包含了不少javax.*的包名。 Java语言口号 Write Once , Run Anywhere。 JDK命名JDK从1.5版本开始，官方在正式文档与宣传上不再使用类似JDK1.5的命名，只在程序内部使用的的开发版本号（Developer Version，例如java –version的输出）中才继续沿用1.5,1.6…。而攻克版本号（Product Version）则改为JDK5、JDK6、JDK7的命名方式。Java开源 2006年11月13日的JavaOne大会上，Sun公司宣布开源java,JDK1.6在12月11日发布。并建立了OpenJDK组织对浙西源码进行独立管理，除了极少量的产权代码（Encumbered Code，这部分代码大多史Sun本身也无权限进行开源处理的）外，OpenJDK几乎包括了Sun JDK的全部代码。Java虚拟机发展史 Sun公司的java虚拟机Sun Classic VM： JDK1.0提供的一个纯解释执行的Java虚拟机。Exact VM:JDK1.2提供，具有两级及时编译器、编译器与解释器混合工作模式，使用了准确式内存管理（Exact Memory Management，也叫Non-Conservative/Accurate Memory Management）而得名，即虚拟机可以知道内存中某个位置的数据具体式什么类型。譬如内存中有一个32位的整数123456，它到底式一个reference类型指向123456的内存地址还是一个数值位123456的整数，虚拟机将有能力分辨出来，这样才能在GC（垃圾收集）的时候准确判断堆上的数据是否还可能被使用。Exact VM抛弃了Classic VM基于handler的对象查找方式（原因是进行GC后对象将可能会被移动位置，如果将地址为123456的对象移动到654321，在没有明确信息表明内存中哪些数据是reference的前提，虚拟机不敢把内存中所有为123456的值改成654321的，所以要使用句柄来保持reference值的稳定），这样每次对象都少了一次间接查找的开销，提升执行性能。HotSpot VM：(目前一直使用)JDK1.2提供，内置了JIT(Just in Time)编译器，继承了前面2款虚拟机的优点如：Exact Memory Management。自己新技术，如其名：HotSpot指的就是它的热点代码探测技术，ExactVM中也有，热点探测可以通过执行计数器找出最具有编译价值的代码，然后通知JIT编译器以方法为单位进行编译。其它公司的Java虚拟机 JRockit VM：BEA公司收购的，并将其发展为一款专门为服务器硬件和服务器端应用场景高度优化的虚拟机，由于不太关注程序启动速度只专注与服务端应用，因此JRockit内部不包含解析器实现，全部代码都靠即使编译器编译执行，JRockit的垃圾收集器和MissionControl服务套件等部分的实现，在众多Java虚拟机中也处于领先。 可以运行在Java虚拟机上的语言 对于这些运行于java虚拟机之上，Java语言之外的语言，来自系统级的、底层底层的支持正在迅速增强，JSR-292为核心的一系列项目和功能改进，推动java虚拟机从“Java语言的虚拟机”向“多语言虚拟机”的方向发展 编写JDK使用的语言 OpenJDK的各个组成部分（Hotspot、JDK API、JAXWS、JAXP….)有的是使用C++编写的，更多的代码则是使用Java自身实现的。","tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}]},{"title":"深入理解Java虚拟机（目录）","date":"2017-03-17T02:48:59.769Z","path":"2017/03/17/深入理解Java虚拟机（目录）/","text":"本书一共分为5个章节，13个小结第一部分 走进Java本书的第一部分为后文的讲解建立了良好的基础，尽管了解Java技术的来龙去脉，以及编译自己的OpenJDK对于读书理解Java虚拟机并不是必需的，但是这些准备可以过程可以为走进Java技术和虚拟机提供良好的引导，第一分部只有第一章： 第一章介绍了Java技术体系的过去，现在和未来的一些发展趋势，并且介绍了如何独立的编译一个OpenJDK 7。 第二部分 自动内存管理机制因为程序员把内存控制的权利交给了Java虚拟机，所以可以在编码的时候享受自动内存管理的诸多优势，不过也正是这个原因，一旦出现内存泄漏和溢出得问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将会成为一项艰难的工作。第二部分有2~5章。 第二章讲解了虚拟机内存是如何划分的，以及那部分区域，什么样的操作和代码可以导致内存溢出异常，讲解了各个区域出现内存溢出异常常见的原因。 第三章分析了垃圾收集的算法和JDK 1.7 中提供的几款垃圾收集器的特点以及运行的原理，通过代码示例验证了Java虚拟机中自动内存分配以及回收的主要规则。 第四章加少了JDK发布的6个命令工具和两个可视化的故障处理工具的使用方法。 第五章与读者分享了几个比较比较有代表性的实际案例。还准备了所有开发人员都能亲身实战的练习，读者可以通过实践来获得故障处理和调优的经验。 第三部分 虚拟机执行子系统执行子系统是虚拟机中必不可少的组成部分，了解了虚拟机如何执行程序，才能写出优秀的代码。第三部分包括6~9章： 第六章讲解了Class文件中的各个组成部分，以及每个部分中的定义，数据结构和使用方法，以实战的方法演示了Class文件的数据是如何储存和访问的。 第七章介绍了类加载过程中的“加载”，“验证”，“准备”，“解析”和“初始化”的5个阶段和虚拟机分别执行了那些动作，还介绍了类加载器的工作原理以及其对虚拟机的意义。 第八章分析了虚拟机在执行代码的时候如何找到正确的方法，如何执行方法内的字节码，以及执行代码时设计的内存结构。 第九章通过四个类加载以及执行子系统的案例，分享了使用类加载器和处理字节码的一些值得欣赏和借鉴的思路，并且通过一些实战练习来加深对前面理论知识的理解。 第四部分Java程序从源码编译到字节码以及从字节码编译成本地机器码的两个过程，合并起来就等同于一个传统编译器所执行的编译过程，第四部分主要包括10~11章 第十章分析了Java语言中的泛型，主动装箱和拆箱，条件编译等多种语法糖的前因后果。并且通过实战演示了如何插入式注解处理器来实现一个检查程序命名规范的编译器插件。 第十一章讲解了虚拟机的热点探测方法，HotSpot的即时编译器。编译触发条件，以及如何从虚拟机外部观察和分析JIT编译的数据和结果，此外，还讲解了几种常见的编译优化技术。 第五部分Java语言和虚拟机提供的原生的，完善的多线程支持，这使得它天生就适合开发多线程并发的应用程序。不过我们不能期望系统来完成所有的并打相关的处理，了解并发的内幕也是一个高级程序员不可缺少的课程，第五部分有12~13章 第十二章讲解了虚拟机Java内存的结构和操作，以及原子性，可见性和有序性在Java内存模型中的体现，介绍了先行发生原则的规则和使用，还了解了线程在Java语言中是如何实现的。 第十三章介绍了线程安全涉及的概念和分类，同步实现的方法以及虚拟机的底层运作原理，并且介绍了虚拟机实现高效并发所采取的一系列的锁优化措施。","tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}]},{"title":"undertow嵌入式web服务器初体验","date":"2017-03-16T02:15:54.205Z","path":"2017/03/16/undertow嵌入式web服务器初体验/","text":"Github源码做的一个项目，在这里开源，删除了大部分业务代码，将逐步改为一个基于netty5的框架，目前只算得上是一个手脚架吧 netty同时处理HTTP和Websocket，并将HTTP请求路由到相应Action中;使用ehcache实现Session；spring IOC做管理容器，mybatis做sql数据库ORM；spring data mongoDB做mongo的ORM；HikariCP做sql数据库连接池；Gson用于json解析和生成；logback日志处理 ####netty处理HTTP和websocket smart.core.netty.HttpHandler：是一个自定义的ChannelHandler用于处理HTTP和Websocket请求 Handler分别处理HTTP和Websocket 1234567public void messageReceived(ChannelHandlerContext ctx, Object msg) &#123; if (msg instanceof FullHttpRequest) &#123;//如果是HTTP请求，进行HTTP操作 handleHttpRequest(ctx, (FullHttpRequest) msg); &#125; else if (msg instanceof WebSocketFrame) &#123;//如果是Websocket请求，则进行websocket操作 handleWebSocketFrame(ctx, (WebSocketFrame) msg); &#125; &#125; 由于websocket也是基于HTTP的，需要判断是websocket后，将HTTP升级为Websocket 12345678910111213141516171819202122private void handleHttpRequest(ChannelHandlerContext ctx, FullHttpRequest req) &#123; logger.warn(\"uri:\" + req.uri()); if (req.uri().startsWith(\"/ws/join\")) &#123;//如果urL开头为/ws/join则升级为websocket mac = wsBeforeHandler(ctx, req); if (mac == null || mac.length() &lt; 1) &#123; RespTools.paraErrorBack(ctx,req,null); return; &#125; WebSocketServerHandshakerFactory wsFactory = new WebSocketServerHandshakerFactory( getWebSocketLocation(req), null, true); handshaker = wsFactory.newHandshaker(req); if (handshaker == null) &#123; WebSocketServerHandshakerFactory.sendUnsupportedVersionResponse(ctx.channel()); &#125; else &#123; handshaker.handshake(ctx.channel(), req); &#125; &#125; else &#123;//是HTTP请求则路由到Action RouteResult&lt;Action&gt; routeResult = rs.getRouter().route(req.method(), req.uri()); Action action = routeResult.target(); action.act(ctx, req); &#125; &#125; websocket请求处理,这里是从websocket请求中获取客户端传来的json字符串，并将字符串转为javabean 12345678910111213141516171819private void handleWebSocketFrame(ChannelHandlerContext ctx, WebSocketFrame frame) &#123; // Check for closing frame if (frame instanceof CloseWebSocketFrame) &#123; handshaker.close(ctx.channel(), (CloseWebSocketFrame) frame.retain()); return; &#125; if (frame instanceof PingWebSocketFrame) &#123; ctx.write(new PongWebSocketFrame(frame.content().retain())); return; &#125; if (frame instanceof TextWebSocketFrame) &#123; devicePool.join(ctx.channel(), mac); String json = ((TextWebSocketFrame) frame).text(); Logic.ReqRespType data= JsonTools.read(json,Logic.ReqRespType.class); //... return; &#125; &#125; 如果是HTTP则需在RouterSetting中配置路由.比如r.POST(“api/get_verify_code”, getVerifyCodeAct):将url为”api/get_verify_code”的POST请求路由到LoginAct中 123456789101112131415161718192021public class RouterSetting &#123; @Autowired private Router&lt;Action&gt; router; @Autowired private GetVerifyCodeAct getVerifyCodeAct;//w @Autowired private LoginAct loginAct; @Autowired private RegisterAct registerAcc; public Router&lt;Action&gt; getRouter() &#123; routerConfig(this.router); return this.router; &#125; private void routerConfig(Router&lt;Action&gt; r) &#123; r.POST(\"api/get_verify_code\", getVerifyCodeAct); r.ANY(\"api/login\", loginAct); r.GET(\"api/register\", registerAcc); &#125;&#125; Action处理HTTP请求并返回 12345678910111213@Controllerpublic class LoginAct implements Action &#123; private static final Logger logger = LoggerFactory.getLogger(LoginAct.class); @Override public void act(ChannelHandlerContext ctx, FullHttpRequest req) &#123; String ip = HttpTools.getIp(req); String body = Convert.buf2Str(req.content()); Get.Login get = JsonTools.read(body, Get.Login.class);//1.得到HTTP传来的json数据解析为javabean Sub.Register back;//构建返回给客户端的javabean的实例 //... HttpTools.sendCorrectResp(ctx, req, back);//返回给客户端HTTP Response &#125;&#125; 添加Session(依靠ehcache) 12345private void addSession(long userId, String ip) &#123; Logic.DeviceSession session = new Logic.DeviceSession(ip, \"\"); Cache.add(userId + \"\", session, \"6mn\");//设置session的缓存时间为6分钟 //debugSession(userId); &#125; 从HTTP请求中获取IP地址1String ip = HttpTools.getIp(req)； ####netty参数设置12345port=8090netty.boss.thread.count=2netty.worker.thread.count=1netty.so.keepalive=truenetty.so.backlog=100 ####项目依赖1234567891011121314151617181920212223242526272829//---------------------单元测试---------------------------- testCompile group: 'junit', name: 'junit', version: '4.11' //--------------------数据库驱动---------------------------- compile 'org.mongodb:mongodb-driver:3.2.2' compile 'mysql:mysql-connector-java:5.1.38' //-------------------数据库连接池--------------------------- compile 'com.zaxxer:HikariCP:2.4.5' //----------------------ORM------------------------------ compile group: 'org.mybatis', name: 'mybatis', version:mybatisVersion compile group: 'org.mybatis', name: 'mybatis-spring', version:mybatisSpringVersion //-----------------------缓存---------------------------- compile group: 'net.sf.ehcache', name: 'ehcache', version:ehcacheVersion //----------------------工具包---------------------------- compile 'commons-httpclient:commons-httpclient:3.1-rc1' compile 'org.javassist:javassist:3.20.0-GA' //---------------------日志处理---------------------------- compile 'org.slf4j:slf4j-api:1.7.21' compile 'ch.qos.logback:logback-core:1.1.7' compile 'ch.qos.logback:logback-classic:1.1.7' //---------------------json处理--------------------------- compile 'com.google.code.gson:gson:2.6.2' //---------------------netty----------------------------- compile group: 'io.netty', name: 'netty-all', version:nettyVersion //---------------------spring---------------------------- compile group: 'org.springframework', name: 'spring-test', version:springVersion compile group: 'org.springframework', name: 'spring-jdbc', version:springVersion compile(group: 'org.springframework', name: 'spring-context', version:springVersion) &#123; exclude(module: 'commons-logging') &#125;","tags":[{"name":"undertow","slug":"undertow","permalink":"http://yoursite.com/tags/undertow/"}]},{"title":"Springboot 整合 Mybatis","date":"2017-03-16T02:14:30.993Z","path":"2017/03/16/Springboot 整合 Mybatis/","text":"为什么使用Mybatis？ Mybatis是目前很火的SSM框架中的ORM组件，相比Hibernate更加灵活小巧，学习成本也更低，我觉得可维护性也更好些。 但是spring boot官方更只提供了自家的spring data jpa及hibernate的整合方案，而没有给出Mybatis的整合组件。于是上Github，发现了Mybatis提供了它的spring-boot-starter。 ##整合方法 gradle中加入依赖1compile(\"org.mybatis.spring.boot:mybatis-spring-boot-starter:1.1.1\") spring.boot:mybatis-spring-boot-starter中已经包含了对mybatis和mybatis-spring的依赖 在application.yml中配置mybatis12345mybatis: #指定mapper和domain(实体)所在的包 type-aliases-package: me.jcala.blog.domain,me.jcala.blog.mapping #指定使用的类型转换器 type-handlers-package: org.apache.ibatis.type.LocalDateTypeHandler 除此之外mybatis还提供了一下配置12345mybatis: config-location: #mybatis的xml注册文件位置 mapper-locations: #Mapper xml config files (optional) executor-type: #执行类型为: SIMPLE, REUSE还是BATCH configuration: #mybatis的其他配置 在Spring Boot中配置好数据源DataSource可以使用任意数据源，mybatis会自动使用spring boot中所配置的数据库连接池 以上就完成了spring boot对mybatis的整合，超级简单啊 测试一下因为在type-aliases-package: me.jcala.blog.domain,me.jcala.blog.mapping中指定的mapping扫描包为me.jcala.blog.mapping，所以要把写的mapper放到me.jcala.blog.mapping包下。 在me.jcala.blog.mapping下新建一个TestMapper接口123456@Repository@Mapperpublic interface TestMapper &#123; @Insert(\"insert into users set username='zzp',password='zzp105'\") void insert();&#125; 再随便写一个测试的类12345678public class TestForMapper&#123;@AutowiredTestMapper testMapper;@Testpublic void testInsert()&#123; testMapper.insert(); &#125;&#125;","tags":[{"name":"springboot mybatis","slug":"springboot-mybatis","permalink":"http://yoursite.com/tags/springboot-mybatis/"}]},{"title":"微服务","date":"2017-03-16T02:03:09.620Z","path":"2017/03/16/微服务/","text":"欢迎使用 {小书匠}(xiaoshujiang)编辑器，您可以通过==设置==里的修改模板来改变新建文章的内容。 微服务一个新架构术语的定义微服务架构是将软件应用程序设计为一套可独立部署的服务组件。虽然在架构风格上没有精确的定义，但在业务功能、自动化部署、端点智能化以及对语言与数据的离散化控制能力上具备通用特征 “微服务” 是在已经人满为患的软件架构领域出现的新名词。虽然我们对于新的事物往往会抱有一种轻视的态度去认识它。但慢慢的会发现这种软件架构风格变得越发的吸引人。过去几年，我们看到许多实际的项目去使用它。应用后的结果至今也是蛮不错的。更有甚一些同事干脆把它当作构建企业级应用程序的首选。遗憾的是，到目前为止没有更多官方的信息指导我们如何在项目中使用它。 简而言之， 微服务架构的风格 1 是将单一的应用程序作为一组小服务套件来开发的一种方法，每个服务都运行在各自的进程中，并利用轻量化机制（通常是HTTP资源API）实现通信。并且围绕着业务能力构建，通过自动化部署机制实现独立部署。这些服务匹配一套最低限度的中央式管理机制，每个服务可通过不同的编程语言编写，使用不同数据存储技术。 对比着单体式架构风格来理解为服务是再好不过了，一个单体式应用作为一个单一的单元进行构建。构建企业级应用程序通常包含一下三个主要部分： 客户端用户界面（由运行在用户设备上的浏览器中的HTML页面以及JavaScript代码构成） 后端数据库（由大量插入至数据库管理系统的表构成，通常采用关系数据库） 服务器端应用程序 服务器端的应用程序将会处理HTTP请求，执行域逻辑。检索与更新数据库。同时选定HTML视图并将其发送至浏览器端。服务器端应用程序通常为单一逻辑的可执行程序 [15]。任何针对该系统的变更都需要对该服务器端应用程序进行新版本构建与部署。 这样的单体服务器机制在构建此类系统中可谓不可或缺。我们用于处理请求的全部逻辑都运行在单一进程当中，允许大家使用语言中的基本功能以将该应用程序拆分为类、函数以及命名空间。通过这种方式，我们能够在开发人员的笔记本设备上运行并测试应用程序，同时利用一整套部署流程以确保全部变更都经过妥善测试而后被部署在生产环境当中。大家可以将大量实例运行在一套负载均衡方案之后，从而实现横向扩展能力。 单体式应用程序当然能够切实起效，但人们却逐渐发现其中存在着诸多弊端，特别是在将大量应用程序部署在云环境当中的情况下。由于变更周期被大量集中于一处，即使仅仅指向应用程序中的一小部分，单一变更亦要求我们对应用程序整体进行重构与重新部署。随着时间推移，我们往往很难保证理想的模块化结构，这意味着本应只影响单一模块的变更往往会扩散至该模块之外。规模伸缩亦要求我们对整体应用程序进行规模调整，而非单纯为其中必要的部分进行资源扩容。 单体应用程序与微服务应用程序图一：单体应用程序与微服务应用程序 正是这些弊端造就了如今的微服务架构风格：即以服务套件的形式构建应用程序。除了各服务能够单独进行部署与规模伸缩之外，每项服务还具备牢固的模块边界，甚至允许我们在不同的服务当中使用不同的编程语言进行代码编写。另外，各服务亦可由不同团队负责管理。 我们认为微服务风格并不算什么新鲜事物或者创新成果，其历史至少可以追溯至Unix设计时代。但我们同时亦坚信，微服务架构一直未能受到足够的重视，而其确实能够帮助大家更好地完成软件开发工作。 微服务架构的特性我们无法给微服务架构风格出具一条确切的定义，但我们却可以根据该架构表现出的各类共同特性对其加以描述。正如各类根据共同特性做出的定义一样，并不是所有微服务架构都符合这些特性，但可以肯定的是具备这些特性的微服务架构占据大部分比例。尽管我们各部分内容的作者仅仅是相关技术社区中的活跃成员，但制作这份文档是为了对采用微服务架构的工作流程及成果做出总结，而且其中仍有相当一部分表述并非严格定义，只应作为常见情况考量。 通过服务实现组件化长久以来，我们一直参与软件行业之内并意识到人们对利用组件整合方式构建系统的渴望——这种思路与我们在物理世界中采取的构建机制非常相似。而在过去几十年当中，我们发现已经有大量公共库渗透到多数语言平台当中并成为其坚实的组成部分。 在谈到我们所使用的组件时，大家可能会发现不同群体对组件的定义也有所区别。我们对组件做出的定义是，其属于软件中的一类单元，且具备可更替性与可升级性。 微服务架构会使用这些库，但其实现组件化的主要手段则是将软件拆分成多个服务。我们将“库”定义为与程序相对接且可通过内存内函数调用发挥作用的组件，而“服务”则为进程之外的组件，其可通过Web服务请求或者远程程序调用等方式实现通信。（这里的服务概念与多数OO程序 3 中的服务对象概念有所区别）。 将服务作为组件加以使用（而非库）的一大原因在于，服务具备独立可部署能力。如果大家的应用程序 4 由单一进程中的多个库 构成，那么指向任何单一组件的变更都会致使该应用程序必须进行重新部署。但如果该应用程序被拆分成多项服务，那么单一服务变更将只会致使该服务进行重新部署。虽然这并非绝对，例如某些变更会导致服务接口受到影响，但一套优秀的微服务架构旨在尽可能少地对服务协议中的服务边界及演进机制产生干扰。 将服务作为组件的另一个理由在于实现更为明确的组件接口。大多数编程语言并不具备用于定义明确发布接口的良好机制。一般来讲，其只会提供说明文档及规则以防止用户打破组件封装，但这同时亦会导致不同组件之间的耦合程度过高。利用明确的远程调用机制，服务能够轻松避免此类难题。 但以这种方式使用服务亦存在一定弊端。远程调用在资源需求方面往往远高于进程内调用，因此远程API需要采取粗粒度设计，但这亦会增加API的使用难度。如果大家需要更改不同组件间的职能分配，那么这类需求在跨越进程边界时往往不易实现。 通过粗略观察，我们往往会发现这些服务会与各运行时进程相映射——但这仅仅只是第一印象。一项服务可能由多个进程构成，且各进程始终共同进行开发与部署——这方面实例包括只由单一服务所使用的应用程序进程以及数据库。 围绕业务功能构建组织当着眼于将单一大型应用程序拆分成多个组成部分时，管理人员通常更重视技术层，其中具体包括UI团队、服务器端逻辑团队以及数据库团队。当这些团队据此进行拆分时，即使是最简单的变更也将给项目造成跨团队协作负担，并因此导致时间与预算的双重支出。睿智的团队会对此进行优化，同时采取两害相权取其轻的办法——即强制要求逻辑存在于一切与之相对接的应用程序当中。换言之，也就是实现逻辑的普遍存在性。这正是所谓康威法则 5 的一种实际表现形式。 任何组织在设计一套系统（广义层面的系统）时，其设计成果都会直接体现该组织所使用的沟通结构。 –梅尔文·康威，1967年 康威定律的实际体现图二：康威定律的实际体现 微服务方案对于各部门而言是一种不同于以往，且以业务功能为核心的服务拆分及组织途径。此类服务采用软件方案在业务层面中的广泛实现堆栈，具体包括用户界面、持久性存储以及任何外部协作机制。因此，各团队将拥有跨职能特性，包括开发过程当中要求的全部技能组合：用户体验、数据库以及项目管理等等。 由团队边界决定的服务边界图三：由团队边界决定的服务边界 采取此类组织方式的企业实例可参见comparethemarket，其各职能团队共同负责构建并运营每款产品，而每款产品则被拆分为一系列独立的服务——且各服务间通 过一套消息收发总线实现通信。 大型整体应用程序亦可以始终围绕业务功能实际模块化，不过这种状况并不常见。诚然，我们都听说过由大型团队构建的单一整体应用程序根据自身业务线进行设计与划分。然而在这类情况下，最大的问题在于整体应用程序在组织当中需要考虑太多背景信息。如果其整体范畴当中包含太多模块边界，那么团队中的单一成员将很难通过短期记忆对其进行管理。除此之外，我们发现这种模块化业务线的维护工作还要求相关人员具备极高的专业技能水平。相比之下，服务组件能够令拆分方式更为明确，从而大大简化团队边界的设定与认知。 微服务架构有多“微”？ 尽管“微服务”早已成为一种极具人气的架构类型，但这一名称却并不能准确反映服务的实际规模——换言之，“微”服务并不一定微。在与众多微服务从业者的交流当中，我们发现服务的具体规模可谓多种多样。其中规模最大的成果源自Amazon公司旗下的“两块披萨”团队（即整个团队只需两块披萨即可填饱肚子），这意味着其总人数在十位左右。而规模较小的团队则由六人组成，负责支持六项服务。那么这就带来了新的问题：这种十二人对单项服务的机制同一人对单项服务之间存在着怎样的差别？二者也许不可一概而论。就目前而言，我们姑且认为双方属于同类团队结构，但随着对微服务认识的持续深入，也许我们未来将抱持新的观点。 产品而非项目大部分应用程序开发工作都会遵循项目模式：其目标在于交付软件方案中的特定部分，并拥有直观的完成指标。在软件开发工作完成后，其会被传递至运维部门，这时负责构建该软件的团队也将即刻解散。 微服务的支持者们则认为这种模式并不可取——他们的主张是相关团队应该伴随产品走过整个生命周期。这方面最典型的例子应该是Amazon公司提出的“谁构建，谁运行”原则，其中开发团队需要对生产环境下的软件成果承担全部责任。这就要求开发人员在日常工作中全程关注其软件的生产运行情况，同时掌握来自用户的反馈意见，意味着他们需要在一定程度上为用户提供技术支持服务。 产品的定位应始终与业务功能相协调。相较于以往将软件视为一整套已经完成的功能集的心态，微服务架构要求我们全程与之保持关联，并思考该软件能够如何协助用户加强业务功能。 当然，我们完全可以将同样的思路引入整体应用程序当中，不过大量小型服务集合能够显著简化服务开发人员与及用户之间的个人联系。 智能化端点与傻瓜式流程在跨越不同进程构建通信结构时，我们发现很多产品及方案会直接把智能化机制塞进通信机制本体当中。这方面的典型实例就是企业服务总线（简称ESB），ESB产品当中通常包含复杂度极高的消息跌幅、编排、转换以及业务规则应用等机制。 微服务社区则倾向于使用另一种实现方式：智能化端点与傻瓜式流程。采用微服务架构的应用程序旨在尽可能实现解耦化与关联性——它们各自拥有自己的域逻辑，而且在经典Unix场景下的运作方式更像是过滤器机制——接收请求、应用合适的逻辑并生成响应。这一切都通过简单的REST类协议实现编排，而非经由WS-Choreography或者BPEL等复杂协议以及中央编排工具实现。 目前最常用的两类协议为配合源API的HTTP请求-响应与轻量化消息收发协议 6 。对于前者，最简练而准确的说明是： 立足于Web，而非居于Web背后。– Ian Robinson 微服务团队采用的正是万维网（在很大程度上亦包括Unix在内）所遵循的原则与协议。一般来讲，其使用的资源能够为开发人员或者运维人员轻松实现缓存处理。 第二类作法则是立足于轻量化消息总线实现消息收发。这类基础设施选项通常具备傻瓜式特性（这种傻瓜特性体现在实现操作上，即只需匹配消息路由机制，再无其它）——以RabbitMQ或者ZeroMQ为代表的简单实现方案仅仅需要提供一套可靠的异步结构，而服务的全部智能化元素仍然存在于端点当中并负责消息的生成与消费。 在整体应用程序当中，各组件在进程内执行并通过方法调用或者函数调用的方式实现彼此通信。将整体应用程序转化为微服务形式的最大难题在于改变这种通信模式。由内存内方法调用指向PC通信机制的简单转换往往无法良好起效。相反，大家需要利用粗粒度方式取代原本的细粒度通信机制。 微服务与SOA当我们探讨微服务时，经常出现的问题就是其到底是不是我们十年前就听说过的面向服务架构（简称SOA）的另一种表现形式？二者之间确实存在一定联系，因为微服务风格拥有与SOA相似的逻辑主张。然而问题在于，SOA的实际含义太过广泛，而且当我们提到所谓“SOA”时，实际所指的对象往往跟这里提到的微服务概念差之千里——具体来讲，其通常代表那些专注于利用ESB实现的集成化整体应用程序。 值得强调的是，我们也见证了大量表现糟糕的面向服务实现手段——从将复杂性隐藏在ESB当中 7 的作法，到投入多年以及数百万资金却毫无成效的尝试，再到以聚合型治理模式抑制变更，我们几乎看不到面向服务架构能够带来什么显著的积极影响。 诚然，微服务社区当中使用的不少技术成果都源自开发人员在大型企业当中积累到的集成化服务成果。Tolerant Reader模式正是其中的典型代表。对Web的运用确实带来可观回报，而使用简单协议正是经验积累的直接产物——这显然是为了解决标准汇聚所导致的高复杂性难题（无论何时，如果大家需要利用一种实体来管理其它实体，那么就意味着各位已经面临着大麻烦）。 SOA的这些弊端导致一部分微服务布道者很讨厌人们把SOA的标签加在微服务头上——尽管也有一些人认为微服务正是SOA的一种实现方式 8，或者说我们可以将微服务称为“面向服务的正确实现”。无论如何，事实上SOA含义的宽泛性意味着其更适合作为一种用于定义架构风格的术语，而非具体解决方案。 离散化治理聚合型治理的一大影响在于使得单一技术平台上出现标准化趋势。经验表明这类方案具备收缩特性——意味着各个实际问题并不能够轻松与解决方案对应起来。我们更倾向于使用正确的工具执行正确的任务，而且虽然部分整体应用程序能够发挥不同编程语言的独特优势，但这种情况并不常见。 通过将整体应用程序的各组件拆分成服务，我们能够对各服务进行分别构建。各位可能希望利用Node.js建立一套简单报告页面？照此办理即可。打算利用C++构建特定的近实时组件？没问题。打算利用不同类型的数据库以匹配单一组件的读取行为？目前的技术方案已经能够实现这种独立重构需求。 当然，我们能够实现以上目标，并不代表我们必须这么做——但对系统进行拆分意味着大家能够拥有更多备用选项。 采用微服务架构的团队倾向于以不同的方式实现所谓标准。相较于以往编写一整套定义标准集的作法，他们更乐于开发实用工具并交付给其他开发人员，从而利用其解决自身面临的类似问题。这些工具通常能够在更为广泛的层面得到实现与共享，但同时又不至于转化为排他性内部开源模式。现在git与github都已经成为客观层面的版本控制系统选项，而开源实践也越来越多地成为内部环境中的常见组成部分。 Netflix公司就是个很好的例子，他们遵循的正是这样一种理念。将具备实用性且经过严格考验的代码作为库，并鼓励其他开发人员利用其以类似的方式解决的类似的问题，这就为各团队成员在必要时选择其它工具保留了空间。共享式库专注于数据存储、进程间通信以及我们在后文中将要探讨的基础设施自动化等问题的解决。 对于微服务社区而言，资源成本显然是种不受欢迎的因素。这并不是说该社区不承认服务协议的价值。恰恰相反，这是因为他们希望构建起大量服务协议。他们希望能够采用多种完全不同的方式对这些协议进行管理。像Tolerant Reader以及Consumer-Driven Contacts这样的模式在微服务架构中非常常见。这些服务协议也各自以独立方式不断演进。将消费者驱动型协议作为构建工作组成部分的作法能够显著增强参与者信心，同时快速获取服务功能能否确切实现的反馈意见。事实上，澳大利亚的某个团队就在积极利用消费者驱动型协议进行新服务构建。他们使用的简单工具确保其能够针对单一服务实现协议定义。其甚至在面向新服务的代码被编写出来之前就已经成为自动化构建流程中的一部分。这意味着服务只有在切实满足该协议要求的前提下才能够实现构建——这就有效解决了构建新软件时经常出现的“YAGNI” [9] 难题。这些技术与工具成果围绕协议而生，并通过降低不同服务间的耦合性限制了其对中央协议管理机制的依赖。 多种语言，多种选项JVM作为平台的快速发展已经成为多种语言混成于单一通用平台内的最新明证。这种作法已经成为一类常见实践，旨在充分发挥高级语言在过去数十年中发展所实现的种种高级抽象优势。其甚至以涓滴效应影响到裸机以及通过低级语言编写的性能敏感型代码。然而，众多整体应用程序并不需要这种级别的性能优化效果，亦非常见的DSL与高级别抽象开发成果。相反，整体应用程序往往使用单一语言，这也严重限制了其能够使用的技术手段。[10] 也许离散化治理的人气正是源自Amazon方面提出的“谁构建，谁运行”原则。各团队需要为其构建的软件的各个方面承担责任，包括为软件提供24/7全天候运维支持。这种程度的责任下放当然还没有成为常态，不过我们已经看到越来越多的企业开始将责任交付至开发团队。Netflix公司亦是另一家采取这种理念 [11] 的企业。为了不至于在凌晨三点被紧急来电叫醒，开发人员们当然会全力以赴提升所编写代码的质量水平。这些思路与传统的集中化治理模式明显相去甚远。 离散化数据管理数据管理离散化拥有多种不同的表现形式。从最为抽象的级别来看，这意味着全局概念模型将在不同系统之间有所区别。这种问题常见于解决方案在大型企业当中的部署，毕竟销售团队对于客户概念的理解方式必须不同于技术支持团队的理解方式。被销售人员视为客户的对象也许根本不会出现的技术支持团队的视野当中。不同属性甚至是相同属性的不同理解方式都可能在语义层面产生细微的差异。 这一问题通常出现在不同应用程序之间甚至是应用程序之内，特别是在将应用程序拆分为多个独立组件的情况下。解决问题的一类可行思路在于基于背景边界化的区域驱动型设计（简称DDD）方案。DDD机制将一个复杂的区域拆分成多个具备边界的背景单元，并对各单元之间的关系加以映射。这种方式同时适用于整体与微服务架构，但服务与背景边界间的自然关联性有助于声明我们曾在业务功能章节中提到过的区分效果。 除了对概念模式进行离散化处理，微服务同时也能够拆分数据存储决策。尽管整体性应用程序倾向于使用单一逻辑数据库保存持久性数据，但企业通常更乐于利用单一数据库涵盖一系列应用程序——而且大多数此类决策立足于具体供应商提供的授权商业模式。微服务机制则选择由每项服务管理其自身数据库的方式，而非不同实例基于同一数据库技术或者完全使用多种不同数据库系统——这种方式亦被称为混合持久化。大家可以利用混合持久化方案打理整体应用程序，但其在微服务架构中的亮相频率明显更高一些。 对微服务架构内数据责任关系的离散化处理也影响到了更新管理工作。常见的更新处理方案是在更新多种资源时，利用事务处理机制来保证其一致性。这种方式通常被用于整体性应用程序汉中。 这种事务处理使用方式确实有助于保障一致性，但却会带来显著的临时性耦合效果，而这在跨越多项服务时会带来新的难题。分布式事务处理非常难以实现，因此微服务架构更强调服务之间的事务处理协调性，同时明确强调只需保障最终一致性并通过补偿运算解决其中的冲突问题。 利用这种方式管理一致性问题已经成为众多开发团队的新困境，但其却能够切实匹配业务实践。一般来讲，企业需要保留一定程度的不一致性以实现某种程度的逆转能力，从而利用快速响应处理错误状况。这种权衡有其必要性，只要确定失误成本要低于高一致性条件下可能造成的业务损失成本即可。 实践性规范与执行标准这种态度实际有点二分法的意味：微服务团队倾向于回避由企业架构部门制定的硬性执行标准，但却乐于使用甚至积极推广HTTP、ATOM以及其它微格式开放标准。 二者之间的本质区别在于标准的开发方式以及执行方式。由IETF等组织管理的标准只会在得到广泛采用之后才能真正成为业界规范，而且其往往脱胎自成功的开源项目。 这些标准拥有与商业世界完全不同的立场与定位——事实上，商业标准的制定工作往往由那些几乎不具备编程经验的团队所负责，或者受到具体厂商的过度影响。 基础设施自动化基础设施自动化技术在过去几年中得到了长足发展——而云与AWS的演进则显著降低了构建、部署及运维微服务架构所带来的复杂性水平。 大部分利用微服务机制构建的产品或者系统都是由具备丰富的持续交付及其前者——持续集成——经验的团队所完成。通过这种方式构建软件的团队能够充分发挥基础设施自动化技术成果的潜在能力。我们可以将整个流程整理成以下图表： 基本构建流程图五：基本构建流程 整体应用程序的构建、测试与推送流程能够在此类环境下顺利完成。事实证明，一旦大家利用自动化流程进行整体应用开发，那么部署更多应用程序也将成为顺理成章的轻松任务。请记住，持续交付的目标之一就是令部署变得无脑化，这意味着无论是一款应用还是三款，其实际部署流程都不会有什么区别 [12]。 我们还发现，不少团队在利用这种广泛的基础设施自动化能力管理生产环境下的微服务架构。相较于前面提到的整体与微服务应用在部署层面并没有太大区别，实际运维环境下的具体条件则存在着巨大差异。 模块部署的具体方式往往差别巨大 图六：模块部署的具体方式往往差别巨大 让正确决定更易于执行 作为一项连带效应，我们发现实现持续交付与部署能够帮助开发人员及运维人员创造出高实用性工具。这类工具能够创建artifact、管理代码库、建立简单服务或者实现标准监控与记录等常见功能。这方面最典型的实例当数Netflix公司发布的一系列开源工具，险些之外Dropwizard等方案亦得到广泛使用。 故障应对设计将服务作为组件加以使用的结果之一在于，应用程序需要经过针对性设计以确保其具备服务故障容错能力。任何服务调用都有可能因为供应程序不可用而发生问题。在这种情况下，客户端必须要尽可能做出适当的回应。相较于整体应用程序来说，服务即组件机制会增加额外的处理复杂性，这也是微服务架构的一大弊端。在这种情况下，微服务团队需要不断审视服务故障对用户体验造成的影响。Netflix公司的“猴子军团”项目就专门负责在正常运营期间对服务进行破坏，甚至利用数据中心故障来测试应用程序的弹性及监控能力。 这类自动化测试机制往往会令正等待周末下班的运维团队们感到不寒而慄。这并不是说整体架构风格就无法使用高复杂性监控机制——只不过这种情况确实不太常见。 由于服务随时可能发生故障，因此最重要的就是保持对故障的快速检测能力，并在可能的情况下对其进行自动恢复。微服务应用程序高度强调对应用程序的实时监控能力，同时不断对架构元素（数据库每秒钟接收到的请求数量）以及业务相关指标（例如每分钟收到的订单数量）进行记录。语义监控能够通过早期预警系统抢先一步做出警示，并引导开发团队对问题加以跟进与调查。 这一点对于微服务架构尤为重要，因为微服务更倾向于采用由编排及事件协作实现的应急处理方式。尽管很多专家都对应急处理方案偶尔带来的收益表示认同，但其实际上往往也是让事情变糟的罪魁祸首。为了及时阻断糟糕的应急处理并确保其拥有可恢复性，监控系统就变得极为重要。 整体应用程序的构建方式可与微服务架构同样透明——事实上也本应如此。二者的区别在于，在面对整体应用时我们需要在确切了解其运行在不同进程中的服务何时发生断开。考虑到同一进程当中可能包含多套库，这种透明度水平实际上很难实现。 微服务团队需要利用复杂的监控与记录机制处理各项服务，例如通过仪表板显示上线/下线状态以及一系列运营与业务相关指标。另外，我们还需要面对断路器状态、当前数据吞吐量以及延迟等其它常见的衡量数据。 演进设计断路器与可交代生产环境之代码 断路器模式出现在Amazon的Release It!当中，其中提到的其它模式还包括隔板模式与超时模式等。在加以结合之后，这些模式将在构建通信应用方面发挥巨大作用。Netflix公司发布的一系列博文就很好地解释了他们对这些模式选项的具体使用方式。 演进设计微服务从业者通常都具备演进设计工作背景，并将服务拆分视为一种深入型工具，旨在帮助应用程序开发人员在无需拖慢变更速度的前提下实现面向应用程序的变更控制。变更控制并不一定意味着变更数量削减——配合正确的态度与工具，大家完全可以帮助软件提供快速、频繁且经过良好控制的变更。 当尝试将一套软件系统拆分为多个组件时，我们往往面临着与具体拆分工作相关的决策任务——即我们应该遵循怎样的方针对应用程序进行拆分？而组件中的关键属性则在于其独立替换与可升级特性 [13] ——这意味着我们要找到确切的平衡点，保证自身能够在不影响其它协作对象的前提下对单一组件进行重写。事实上，很多微服务团队会更进一步，直接清退某些服务而非对其进行长期升级。 英国《卫报》网站就是个很好的例子，其应用程序在设计与构建方面作为整体应用存在，但却在逐步面向微服务架构演进。该网站的核心部分仍然属于整体性项目，但他们更倾向于通过构建微服务利用整体API实现新功能添加。这套方案对于临时性功能的实现非常重要，例如加设专题页面以显示体育赛事报道。网站中的这类组成部分能够通过快速开发语言在短时间内编写完成，并在对应事件结束后立即下线。我们还发现其它一些金融机构亦采取类似的方式公布突发性市场波动，并在数周或者数月之后将其下线。 这也强调了可替换性在模块化设计中的重要地位，其主旨正在于将模块机制贯彻整个变更模式 [14] 。大家希望只变更其中必须变更的部分，而其它模块则继续保持原样。系统当中那些几乎很少变动的部分应该立足于不同于高变更频率组件的服务。如果大家发现自己经常需要同时对两项服务做出变更，那么明显应该将二者加以合并。 将组件纳入服务也让我们能够以更高的细粒度水平进行规划制定。在整体应用程序当中，任何一项变更都需要对应用整体进行重构与重新部署。但在微服务架构方面，我们只需要重新部署包含对应变更的服务。这能够显著简化并加快发布流程。不过其弊端在于，我们必须考虑针对单一服务的变更是否会影响到其它服务。传统的整体性方案能够通过版本控制解决这类难题，但微服务领域则倾向于将版本控制作为最后一种应急办法。我们可以通过设计保证服务拥有强大的容错能力，从而应对其供应程序中出现的各类代码修改。 同步调用殊不可取 无论何时时，一旦在不同服务之间进行多次同步调用，那么可能引发宕机的概率也会以乘法形式增长。简单来讲，系统的总体宕机时间为各单个部件宕机时间的乘积。这时我们就面临着具体选择，到底是以异步方式进行调用，还是以计划方式管理由同步调用带来的宕机时间。英国《卫报》网站在其全新平台上执行了一项简单的规则——每个用户请求对应一次同步调用，而Netflix公司所使用的API则经历重新设计，确保其结构内采用异步调用机制。 微服务是否代表着未来？我们撰写这篇文章的主要目的在于解释微服务架构的基本思路与原则。而在撰写过程当中，我们明确意识到微服务架构风格确实是一项值得重视的关键成果——企业级应用程序开发人员应当对其加以了解。我们最近利用该架构构建了多套系统，而且了解到亦有其它多家企业将其纳入业务体系。 我们了解到的微服务架构先驱企业包括Amazon、Netflix、英国《卫报》、英国政府数字化服务局、realestate.com.au、Forward以及comparethemarket.com等等。2013年召开的相关会议则公布了更多参与其中的重要厂商。除此之外，另有相当一部分企业一直在使用类似的实现思路——但却并没有使用‘微服务’这样的称谓。（其通常将其冠以SOA标签——不过正如我们之前提到，SOA是一类存在大量矛盾取向的概念组合。[15] ） 尽管拥有这些积极的经验，但我们仍然无法完全肯定微服务架构就代表着软件未来的发展方向。虽然我们的实际经历证明微服务架构截至目前仍拥有优于整体性应用程序的积极优势，但必须承认只有充分的时间积累才能帮助我们做出真正完整则准确的判断结论。 我们的同事Sam Newman曾于2014年倾尽心力撰写出这本关于我们如何构建微服务架构类应用的论著。如果大家希望进一步探讨这个议题，请千万不要错过。 通常来说，架构决策的实际影响可能需要几年之后才能逐步显现出来。我们已经看到不少优秀的团队带着巨大的热情与愿景而投入工作，但最终却构建起一套陈旧不堪的整体性架构。很多人认为同样的情况不太可能发生在微服务架构身上，因为其服务边界非常明确因此不太可能发生相互影响。但由于时间尚短且系统程度不足，我们目前还无法真正评估微服务架构的成熟度水平。 人们对微服务成熟度抱持的怀疑态度也有其理由。在任何组件化尝试工作当中，最终结果的成功与否都取决于该软件与拆分后组件的契合效果。我们目前仍然很难说明组件边界的选择原则。演进设计导致边界划分变得非常困难，因此最重要的是保证其重构的简易性。但一旦将组件作为服务处理以实现远程通信，那么其重构难度将远远高于进程内库。在不同服务边界之间进行代码移动难度极大，而任何接口变更都需要在不同相关服务间实现，同时添加层的向下兼容能力，这无疑会令测试工作更加复杂。 另一大问题在于，如果相关组件间的关系不够简洁，那么我们就相当于把组件内部的复杂性转移到了不同组件间的连接当中。这样做不仅会导致复杂性扩散，同时亦会导致其明确性缺失且难以控制。立足于小型、简单组件审视问题总是更为直观，而在不同服务间进行纵览则往往会错失关注点。 最后，团队的技能水平也将起到决定性作用。新型技术成果往往要求高水平技术团队加以实施。不过高水平团队能够顺畅利用的技术方案并不一定能够在低水平人员手中发挥作用。我们已经见证了众多低水平团队构建起的如一团乱麻般的整体架构，但仍需要时间来了解微服务架构是否会在同样的情况下引发同样的状况。诚然，糟糕的团队创建出的始终只能是糟糕的系统——但我们不知道微服务架构到底是会缓解这种状况，还是令状况更中惨不忍睹。 目前有一种较为理性的论调，认为我们不应将微服务架构作为起步方案。相反，大家可以从整体性开发风格出发，保证其结合模块化机制，并在整体性特征引发实际问题后逐步将其拆分为微服务形式。（不过这样的建议并非完全理想，因为良好的进程内接口往往并不能成为良好的服务接口。） 因此我们对此抱持谨慎的乐观态度。到目前为止，我们已经了解到关于微服务架构的方方面面，而且其应该能够成为一种极具价值的开发手段。虽然还不能做出最终判断，但软件开发工作的固有挑战之一，正是我们只能根据目前掌握的远称不上完美的信息做出决策。 脚注1: “微服务”一词最早被威尼斯附近的一个软件架构师小组于2011年5月首次提及，当时他们用这个词汇来描述自己近期研究项目当中所涉及的通用性架构机制。2012年5月，该小组作出最终决议，认为“微服务”是最适合的架构名称。2012年3月，James在《微服务-Java以及Unix方式》当中就此发表了一篇案例研究报告，而Fred George也几乎在同一时间进行了相同的工作。Netflix公司的Adrian Cockcroft将微服务架构称为“细化SOA”，并认为这是一套在Web规模下具备开创意义的架构类型。Joe Walnes、Dan North、Evan Botcher以及Graham Tackley也分别在这篇文章中对此作出了评论。 2: 文章中所使用的“整体”一词长久以来一直被Unix业界所使用。其首次出现在《Unix编程艺术》一书中，用于描述那些过于庞大的系统方案。 3: 很多面向对象设计人员，也包括我们自己，都会在域驱动设计当中使用“服务对象”这一表述，专指那些并不具备实质性联系但却拥有重要作用的对象。这与我们在本文中所使用的“服务”一词在表意上完全不同。遗憾的是，服务这个词汇同时具备两种含义，而我们对这种多义词也没有更好的处理办法。 4: 我们将一款应用程序视为一套社会性体系，其中融合了代码库、函数组以及供应主体。 5: 大家可以查看梅尔文 康韦网站上的原文论述。 6: 对于规模极为庞大的应用体系，企业通常会采用二进制协议——例如protobufs。使用二进制协议的系统仍然符合智能化端点与傻瓜式通道的特性——并为了规模化而在透明度方面作出妥协。不过大多数Web方案与绝大多数企业不需要在这方面考虑太多——一般来讲，透明度越高、效果就越好。 7: 虽然无关紧要，但Jim Webber曾经将ESB解释成“Egregious Spaghetti Box”，也就是“恐怖意面盒”。 8: Netflix公司最近将其架构类型称为“细化SOA”。 9: “YAGNI”的全称是“You Aren’t Going To Need It（你根本不需要它）”，这是一项经典的用户体验原则，即不要自作聪明地添加非必要性功能。 10: 我们所宣称的整体型应用只支持单一语言确实有些不尽不实——在当下的Web系统构建过程中，大家可能需要掌握JavaScript、XHTML以及CSS，而在服务器端的语言选项则包括SQL以及某种ORM（即对象关系映射）衍生语言。没错，单一语言肯定玩不转，但我相信大家明白我想要强调的意思。 11: Adrian Cockcroft在2013年11月的Flowcon大会上作出了精彩演讲，并特别提到了“开发者自助服务”与“开发者应亲自运行所编写代码”的观点。 12: 我们在这里的说法并不准确。很明显，在更为复杂的拓扑结构中部署大量服务肯定要比在单一整体型架构内进行部署困难得多。幸运的是，各类模式能够显著降低这种复杂性——当然，在工具方面的投入仍然不可或缺。 13: 事实上，Dan North将这种类型称为“可替代式组件架构”而非微服务架构。由于其强调内容属于微服务架构的一类子集，所以我们更倾向于使用后一种表达方式。 14: Kent Beck将此作为其《实施模式》一文中的设计原则之一。 15: SOA几乎是此类架构的历史起源。我记得当SOA一词在本世纪初刚刚出现时，很多人表示“我们几年前就已经将其引入日常工作了”。也有意见认为这种架构类型似乎最早出现于早期企业计算当中，COBOL程序通过数据文件实现通信的处理机制。而在另一方面，也有人认为微服务架构与Erlang编程模型其实是同一回事，不过后者只被应用在企业应用程序当中。 Microservicesa definition of this new architectural term The term “Microservice Architecture” has sprung up over the last few years to describe a particular way of designing software applications as suites of independently deployable services. While there is no precise definition of this architecural style, there are certain commom characteristics around organization around business capablity, automated deployment, intelligence in the endpoints, and decentralized control of languages and data. Microservices “Microservices” - yet another new term on the crowded streets of software architecture. Although our natural inclination is to pass such things by with a contemptuous glance, this bit of terminology describes a style of software systems that we are finding more and more appealing. We’ve seen many projects use this style in the last few years, and results so far have been positive, so much so that for many of our colleagues this is becoming the default style for building enterprise applications. Sadly, however, there’s not much information that outlines what the microservice style is and how to do it. In short, the microservice architectural style 1 is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies. To start explaining the microservice style it’s useful to compare it to the monolithic style: a monolithic application built as a single unit. Enterprise Applications are often built in three main parts: a client-side user interface (consisting of HTML pages and javascript running in a browser on the user’s machine) a database (consisting of many tables inserted into a common, and usually relational, database management system), and a server-side application. The server-side application will handle HTTP requests, execute domain logic, retrieve and update data from the database, and select and populate HTML views to be sent to the browser. This server-side application is a monolith - a single logical executable 2. Any changes to the system involve building and deploying a new version of the server-side application. Such a monolithic server is a natural way to approach building such a system. All your logic for handling a request runs in a single process, allowing you to use the basic features of your language to divide up the application into classes, functions, and namespaces. With some care, you can run and test the application on a developer’s laptop, and use a deployment pipeline to ensure that changes are properly tested and deployed into production. You can horizontally scale the monolith by running many instances behind a load-balancer. Monolithic applications can be successful, but increasingly people are feeling frustrations with them - especially as more applications are being deployed to the cloud . Change cycles are tied together - a change made to a small part of the application, requires the entire monolith to be rebuilt and deployed. Over time it’s often hard to keep a good modular structure, making it harder to keep changes that ought to only affect one module within that module. Scaling requires scaling of the entire application rather than parts of it that require greater resource. Monoliths and MicroservicesFigure 1: Monoliths and Microservices These frustrations have led to the microservice architectural style: building applications as suites of services. As well as the fact that services are independently deployable and scalable, each service also provides a firm module boundary, even allowing for different services to be written in different programming languages. They can also be managed by different teams . We do not claim that the microservice style is novel or innovative, its roots go back at least to the design principles of Unix. But we do think that not enough people consider a microservice architecture and that many software developments would be better off if they used it. Characteristics of a Microservice ArchitectureWe cannot say there is a formal definition of the microservices architectural style, but we can attempt to describe what we see as common characteristics for architectures that fit the label. As with any definition that outlines common characteristics, not all microservice architectures have all the characteristics, but we do expect that most microservice architectures exhibit most characteristics. While we authors have been active members of this rather loose community, our intention is to attempt a description of what we see in our own work and in similar efforts by teams we know of. In particular we are not laying down some definition to conform to. Componentization via Services For as long as we’ve been involved in the software industry, there’s been a desire to build systems by plugging together components, much in the way we see things are made in the physical world. During the last couple of decades we’ve seen considerable progress with large compendiums of common libraries that are part of most language platforms. When talking about components we run into the difficult definition of what makes a component. Our definition is that a component is a unit of software that is independently replaceable and upgradeable. Microservice architectures will use libraries, but their primary way of componentizing their own software is by breaking down into services. We define libraries as components that are linked into a program and called using in-memory function calls, while services are out-of-process components who communicate with a mechanism such as a web service request, or remote procedure call. (This is a different concept to that of a service object in many OO programs 3.) One main reason for using services as components (rather than libraries) is that services are independently deployable. If you have an application 4 that consists of a multiple libraries in a single process, a change to any single component results in having to redeploy the entire application. But if that application is decomposed into multiple services, you can expect many single service changes to only require that service to be redeployed. That’s not an absolute, some changes will change service interfaces resulting in some coordination, but the aim of a good microservice architecture is to minimize these through cohesive service boundaries and evolution mechanisms in the service contracts. Another consequence of using services as components is a more explicit component interface. Most languages do not have a good mechanism for defining an explicit Published Interface. Often it’s only documentation and discipline that prevents clients breaking a component’s encapsulation, leading to overly-tight coupling between components. Services make it easier to avoid this by using explicit remote call mechanisms. Using services like this does have downsides. Remote calls are more expensive than in-process calls, and thus remote APIs need to be coarser-grained, which is often more awkward to use. If you need to change the allocation of responsibilities between components, such movements of behavior are harder to do when you’re crossing process boundaries. At a first approximation, we can observe that services map to runtime processes, but that is only a first approximation. A service may consist of multiple processes that will always be developed and deployed together, such as an application process and a database that’s only used by that service. Organized around Business Capabilities When looking to split a large application into parts, often management focuses on the technology layer, leading to UI teams, server-side logic teams, and database teams. When teams are separated along these lines, even simple changes can lead to a cross-team project taking time and budgetary approval. A smart team will optimise around this and plump for the lesser of two evils - just force the logic into whichever application they have access to. Logic everywhere in other words. This is an example of Conway’s Law 5 in action. Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of &gt; the organization’s communication structure. – Melvyn Conway, 1967 Service boundaries reinforced by team boundariesFigure 2: Conway’s Law in action The microservice approach to division is different, splitting up into services organized around business capability. Such services take a broad-stack implementation of software for that business area, including user-interface, persistant storage, and any external collaborations. Consequently the teams are cross-functional, including the full range of skills required for the development: user-experience, database, and project management. Service boundaries reinforced by team boundariesFigure 3: Service boundaries reinforced by team boundaries One company organised in this way is comparethemarket. Cross functional teams are responsible for building and operating each product and each product is split out into a number of individual services communicating via a message bus. Large monolithic applications can always be modularized around business capabilities too, although that’s not the common case. Certainly we would urge a large team building a monolithic application to divide itself along business lines. The main issue we have seen here, is that they tend to be organised around too many contexts. If the monolith spans many of these modular boundaries it can be difficult for individual members of a team to fit them into their short-term memory. Additionally we see that the modular lines require a great deal of discipline to enforce. The necessarily more explicit separation required by service components makes it easier to keep the team boundaries clear. How big is a microservice? Although “microservice” has become a popular name for this architectural style, its name does lead to an unfortunate focus on the size of service, and arguments about what constitutes “micro”. In our conversations with microservice practitioners, we see a range of sizes of services. The largest sizes reported follow Amazon’s notion of the Two Pizza Team (i.e. the whole team can be fed by two pizzas), meaning no more than a dozen people. On the smaller size scale we’ve seen setups where a team of half-a-dozen would support half-a-dozen services. This leads to the question of whether there are sufficiently large differences within this size range that the service-per-dozen-people and service-per-person sizes shouldn’t be lumped under one microservices label. At the moment we think it’s better to group them together, but it’s certainly possible that we’ll change our mind as we explore this style further. Products not Projects Most application development efforts that we see use a project model: where the aim is to deliver some piece of software which is then considered to be completed. On completion the software is handed over to a maintenance organization and the project team that built it is disbanded. Microservice proponents tend to avoid this model, preferring instead the notion that a team should own a product over its full lifetime. A common inspiration for this is Amazon’s notion of “you build, you run it” where a development team takes full responsibility for the software in production. This brings developers into day-to-day contact with how their software behaves in production and increases contact with their users, as they have to take on at least some of the support burden. The product mentality, ties in with the linkage to business capabilities. Rather than looking at the software as a set of functionality to be completed, there is an on-going relationship where the question is how can software assist its users to enhance the business capability. There’s no reason why this same approach can’t be taken with monolithic applications, but the smaller granularity of services can make it easier to create the personal relationships between service developers and their users. Smart endpoints and dumb pipes When building communication structures between different processes, we’ve seen many products and approaches that stress putting significant smarts into the communication mechanism itself. A good example of this is the Enterprise Service Bus (ESB), where ESB products often include sophisticated facilities for message routing, choreography, transformation, and applying business rules. The microservice community favours an alternative approach: smart endpoints and dumb pipes. Applications built from microservices aim to be as decoupled and as cohesive as possible - they own their own domain logic and act more as filters in the classical Unix sense - receiving a request, applying logic as appropriate and producing a response. These are choreographed using simple RESTish protocols rather than complex protocols such as WS-Choreography or BPEL or orchestration by a central tool. The two protocols used most commonly are HTTP request-response with resource API’s and lightweight messaging 6. The best expression of the first is Be of the web, not behind the web – Ian Robinson Microservice teams use the principles and protocols that the world wide web (and to a large extent, Unix) is built on. Often used resources can be cached with very little effort on the part of developers or operations folk. The second approach in common use is messaging over a lightweight message bus. The infrastructure chosen is typically dumb (dumb as in acts as a message router only) - simple implementations such as RabbitMQ or ZeroMQ don’t do much more than provide a reliable asynchronous fabric - the smarts still live in the end points that are producing and consuming messages; in the services. In a monolith, the components are executing in-process and communication between them is via either method invocation or function call. The biggest issue in changing a monolith into microservices lies in changing the communication pattern. A naive conversion from in-memory method calls to RPC leads to chatty communications which don’t perform well. Instead you need to replace the fine-grained communication with a coarser -grained approach. Microservices and SOA When we’ve talked about microservices a common question is whether this is just Service Oriented Architecture (SOA) that we saw a decade ago. There is merit to this point, because the microservice style is very similar to what some advocates of SOA have been in favor of. The problem, however, is that SOA means too many different things, and that most of the time that we come across something called “SOA” it’s significantly different to the style we’re describing here, usually due to a focus on ESBs used to integrate monolithic applications. In particular we have seen so many botched implementations of service orientation - from the tendency to hide complexity away in ESB’s 7, to failed multi-year initiatives that cost millions and deliver no value, to centralised governance models that actively inhibit change, that it is sometimes difficult to see past these problems. Certainly, many of the techniques in use in the microservice community have grown from the experiences of developers integrating services in large organisations. The Tolerant Reader pattern is an example of this. Efforts to use the web have contributed, using simple protocols is another approach derived from these experiences - a reaction away from central standards that have reached a complexity that is, frankly, breathtaking. (Any time you need an ontology to manage your ontologies you know you are in deep trouble.) This common manifestation of SOA has led some microservice advocates to reject the SOA label entirely, although others consider microservices to be one form of SOA 8, perhaps service orientation done right. Either way, the fact that SOA means such different things means it’s valuable to have a term that more crisply defines this architectural style. Decentralized Governance One of the consequences of centralised governance is the tendency to standardise on single technology platforms. Experience shows that this approach is constricting - not every problem is a nail and not every solution a hammer. We prefer using the right tool for the job and while monolithic applications can take advantage of different languages to a certain extent, it isn’t that common. Splitting the monolith’s components out into services we have a choice when building each of them. You want to use Node.js to standup a simple reports page? Go for it. C++ for a particularly gnarly near-real-time component? Fine. You want to swap in a different flavour of database that better suits the read behaviour of one component? We have the technology to rebuild him. Of course, just because you can do something, doesn’t mean you should - but partitioning your system in this way means you have the option. Teams building microservices prefer a different approach to standards too. Rather than use a set of defined standards written down somewhere on paper they prefer the idea of producing useful tools that other developers can use to solve similar problems to the ones they are facing. These tools are usually harvested from implementations and shared with a wider group, sometimes, but not exclusively using an internal open source model. Now that git and github have become the de facto version control system of choice, open source practices are becoming more and more common in-house . Netflix is a good example of an organisation that follows this philosophy. Sharing useful and, above all, battle-tested code as libraries encourages other developers to solve similar problems in similar ways yet leaves the door open to picking a different approach if required. Shared libraries tend to be focused on common problems of data storage, inter-process communication and as we discuss further below, infrastructure automation. For the microservice community, overheads are particularly unattractive. That isn’t to say that the community doesn’t value service contracts. Quite the opposite, since there tend to be many more of them. It’s just that they are looking at different ways of managing those contracts. Patterns like Tolerant Reader and Consumer-Driven Contracts are often applied to microservices. These aid service contracts in evolving independently. Executing consumer driven contracts as part of your build increases confidence and provides fast feedback on whether your services are functioning. Indeed we know of a team in Australia who drive the build of new services with consumer driven contracts. They use simple tools that allow them to define the contract for a service. This becomes part of the automated build before code for the new service is even written. The service is then built out only to the point where it satisfies the contract - an elegant approach to avoid the ‘YAGNI’ [9] dilemma when building new software. These techniques and the tooling growing up around them, limit the need for central contract management by decreasing the temporal coupling between services. Many languages, many options The growth of JVM as a platform is just the latest example of mixing languages within a common platform. It’s been common practice to shell-out to a higher level language to take advantage of higher level abstractions for decades. As is dropping down to the metal and writing performance sensitive code in a lower level one. However, many monoliths don’t need this level of performance optimisation nor are DSL’s and higher level abstractions that common (to our dismay). Instead monoliths are usually single language and the tendency is to limit the number of technologies in use [10]. Perhaps the apogee of decentralised governance is the build it / run it ethos popularised by Amazon. Teams are responsible for all aspects of the software they build including operating the software 24/7. Devolution of this level of responsibility is definitely not the norm but we do see more and more companies pushing responsibility to the development teams. Netflix is another organisation that has adopted this ethos [11]. Being woken up at 3am every night by your pager is certainly a powerful incentive to focus on quality when writing your code. These ideas are about as far away from the traditional centralized governance model as it is possible to be. Decentralized Data Management Decentralization of data management presents in a number of different ways. At the most abstract level, it means that the conceptual model of the world will differ between systems. This is a common issue when integrating across a large enterprise, the sales view of a customer will differ from the support view. Some things that are called customers in the sales view may not appear at all in the support view. Those that do may have different attributes and (worse) common attributes with subtly different semantics. This issue is common between applications, but can also occur within applications, particular when that application is divided into separate components. A useful way of thinking about this is the Domain-Driven Design notion of Bounded Context. DDD divides a complex domain up into multiple bounded contexts and maps out the relationships between them. This process is useful for both monolithic and microservice architectures, but there is a natural correlation between service and context boundaries that helps clarify, and as we describe in the section on business capabilities, reinforce the separations. As well as decentralizing decisions about conceptual models, microservices also decentralize data storage decisions. While monolithic applications prefer a single logical database for persistant data, enterprises often prefer a single database across a range of applications - many of these decisions driven through vendor’s commercial models around licensing. Microservices prefer letting each service manage its own database, either different instances of the same database technology, or entirely different database systems - an approach called Polyglot Persistence. You can use polyglot persistence in a monolith, but it appears more frequently with microservices. Decentralizing responsibility for data across microservices has implications for managing updates. The common approach to dealing with updates has been to use transactions to guarantee consistency when updating multiple resources. This approach is often used within monoliths. Using transactions like this helps with consistency, but imposes significant temporal coupling, which is problematic across multiple services. Distributed transactions are notoriously difficult to implement and and as a consequence microservice architectures emphasize transactionless coordination between services, with explicit recognition that consistency may only be eventual consistency and problems are dealt with by compensating operations. Choosing to manage inconsistencies in this way is a new challenge for many development teams, but it is one that often matches business practice. Often businesses handle a degree of inconsistency in order to respond quickly to demand, while having some kind of reversal process to deal with mistakes. The trade-off is worth it as long as the cost of fixing mistakes is less than the cost of lost business under greater consistency. Battle-tested standards and enforced standards It’s a bit of a dichotomy that microservice teams tend to eschew the kind of rigid enforced standards laid down by enterprise architecture groups but will happily use and even evangelise the use of open standards such as HTTP, ATOM and other microformats. The key difference is how the standards are developed and how they are enforced. Standards managed by groups such as the IETF only become standards when there are several live implementations of them in the wider world and which often grow from successful open-source projects. These standards are a world apart from many in a corporate world, which are often developed by groups that have little recent programming experience or overly influenced by vendors. Infrastructure Automation Infrastructure automation techniques have evolved enormously over the last few years - the evolution of the cloud and AWS in particular has reduced the operational complexity of building, deploying and operating microservices. Many of the products or systems being build with microservices are being built by teams with extensive experience of Continuous Delivery and it’s precursor, Continuous Integration. Teams building software this way make extensive use of infrastructure automation techniques. This is illustrated in the build pipeline shown below. basic build pipelineFigure 5: basic build pipeline Since this isn’t an article on Continuous Delivery we will call attention to just a couple of key features here. We want as much confidence as possible that our software is working, so we run lots of automated tests. Promotion of working software ‘up’ the pipeline means we automate deployment to each new environment. A monolithic application will be built, tested and pushed through these environments quite happlily. It turns out that once you have invested in automating the path to production for a monolith, then deploying more applications doesn’t seem so scary any more. Remember, one of the aims of CD is to make deployment boring, so whether its one or three applications, as long as its still boring it doesn’t matter [12]. Another area where we see teams using extensive infrastructure automation is when managing microservices in production. In contrast to our assertion above that as long as deployment is boring there isn’t that much difference between monoliths and microservices, the operational landscape for each can be strikingly different. Module deployment often differsFigure 6: Module deployment often differs Make it easy to do the right thing One side effect we have found of increased automation as a consequence of continuous delivery and deployment is the creation of useful tools to help developers and operations folk. Tooling for creating artefacts, managing codebases, standing up simple services or for adding standard monitoring and logging are pretty common now. The best example on the web is probably Netflix’s set of open source tools, but there are others including Dropwizard which we have used extensively. Design for failure A consequence of using services as components, is that applications need to be designed so that they can tolerate the failure of services. Any service call could fail due to unavailability of the supplier, the client has to respond to this as gracefully as possible. This is a disadvantage compared to a monolithic design as it introduces additional complexity to handle it. The consequence is that microservice teams constantly reflect on how service failures affect the user experience. Netflix’s Simian Army induces failures of services and even datacenters during the working day to test both the application’s resilience and monitoring. This kind of automated testing in production would be enough to give most operation groups the kind of shivers usually preceding a week off work. This isn’t to say that monolithic architectural styles aren’t capable of sophisticated monitoring setups - it’s just less common in our experience. Since services can fail at any time, it’s important to be able to detect the failures quickly and, if possible, automatically restore service. Microservice applications put a lot of emphasis on real-time monitoring of the application, checking both architectural elements (how many requests per second is the database getting) and business relevant metrics (such as how many orders per minute are received). Semantic monitoring can provide an early warning system of something going wrong that triggers development teams to follow up and investigate. This is particularly important to a microservices architecture because the microservice preference towards choreography and event collaboration leads to emergent behavior. While many pundits praise the value of serendipitous emergence, the truth is that emergent behavior can sometimes be a bad thing. Monitoring is vital to spot bad emergent behavior quickly so it can be fixed. Monoliths can be built to be as transparent as a microservice - in fact, they should be. The difference is that you absolutely need to know when services running in different processes are disconnected. With libraries within the same process this kind of transparency is less likely to be useful. Microservice teams would expect to see sophisticated monitoring and logging setups for each individual service such as dashboards showing up/down status and a variety of operational and business relevant metrics. Details on circuit breaker status, current throughput and latency are other examples we often encounter in the wild. The circuit breaker and production ready code Circuit Breaker appears in Release It!alongside other patterns such as Bulkhead and Timeout. Implemented together, these patterns are crucially important when building communicating applications. This Netflix blog entry does a great job of explaining their application of them. Evolutionary Design Microservice practitioners, usually have come from an evolutionary design background and see service decomposition as a further tool to enable application developers to control changes in their application without slowing down change. Change control doesn’t necessarily mean change reduction - with the right attitudes and tools you can make frequent, fast, and well-controlled changes to software. Whenever you try to break a software system into components, you’re faced with the decision of how to divide up the pieces - what are the principles on which we decide to slice up our application? The key property of a component is the notion of independent replacement and upgradeability [13] - which implies we look for points where we can imagine rewriting a component without affecting its collaborators. Indeed many microservice groups take this further by explicitly expecting many services to be scrapped rather than evolved in the longer term. The Guardian website is a good example of an application that was designed and built as a monolith, but has been evolving in a microservice direction. The monolith still is the core of the website, but they prefer to add new features by building microservices that use the monolith’s API. This approach is particularly handy for features that are inherently temporary, such as specialized pages to handle a sporting event. Such a part of the website can quickly be put together using rapid development languages, and removed once the event is over. We’ve seen similar approaches at a financial institution where new services are added for a market opportunity and discarded after a few months or even weeks. This emphasis on replaceability is a special case of a more general principle of modular design, which is to drive modularity through the pattern of change [14]. You want to keep things that change at the same time in the same module. Parts of a system that change rarely should be in different services to those that are currently undergoing lots of churn. If you find yourself repeatedly changing two services together, that’s a sign that they should be merged. Putting components into services adds an opportunity for more granular release planning. With a monolith any changes require a full build and deployment of the entire application. With microservices, however, you only need to redeploy the service(s) you modified. This can simplify and speed up the release process. The downside is that you have to worry about changes to one service breaking its consumers. The traditional integration approach is to try to deal with this problem using versioning, but the preference in the microservice world is to only use versioning as a last resort. We can avoid a lot of versioning by designing services to be as tolerant as possible to changes in their suppliers. Synchronous calls considered harmful Any time you have a number of synchronous calls between services you will encounter the multiplicative effect of downtime. Simply, this is when the downtime of your system becomes the product of the downtimes of the individual components. You face a choice, making your calls asynchronous or managing the downtime. At www.guardian.co.uk they have implemented a simple rule on the new platform - one synchronous call per user request while at Netflix, their platform API redesign has built asynchronicity into the API fabric. Are Microservices the Future?Our main aim in writing this article is to explain the major ideas and principles of microservices. By taking the time to do this we clearly think that the microservices architectural style is an important idea - one worth serious consideration for enterprise applications. We have recently built several systems using the style and know of others who have used and favor this approach. Those we know about who are in some way pioneering the architectural style include Amazon, Netflix, The Guardian, the UK Government Digital Service, realestate.com.au, Forward and comparethemarket.com. The conference circuit in 2013 was full of examples of companies that are moving to something that would class as microservices - including Travis CI. In addition there are plenty of organizations that have long been doing what we would class as microservices, but without ever using the name. (Often this is labelled as SOA - although, as we’ve said, SOA comes in many contradictory forms. [15]) Despite these positive experiences, however, we aren’t arguing that we are certain that microservices are the future direction for software architectures. While our experiences so far are positive compared to monolithic applications, we’re conscious of the fact that not enough time has passed for us to make a full judgement. Often the true consequences of your architectural decisions are only evident several years after you made them. We have seen projects where a good team, with a strong desire for modularity, has built a monolithic architecture that has decayed over the years. Many people believe that such decay is less likely with microservices, since the service boundaries are explicit and hard to patch around. Yet until we see enough systems with enough age, we can’t truly assess how microservice architectures mature. There are certainly reasons why one might expect microservices to mature poorly. In any effort at componentization, success depends on how well the software fits into components. It’s hard to figure out exactly where the component boundaries should lie. Evolutionary design recognizes the difficulties of getting boundaries right and thus the importance of it being easy to refactor them. But when your components are services with remote communications, then refactoring is much harder than with in-process libraries. Moving code is difficult across service boundaries, any interface changes need to be coordinated between participants, layers of backwards compatibility need to be added, and testing is made more complicated. Another issue is If the components do not compose cleanly, then all you are doing is shifting complexity from inside a component to the connections between components. Not just does this just move complexity around, it moves it to a place that’s less explicit and harder to control. It’s easy to think things are better when you are looking at the inside of a small, simple component, while missing messy connections between services. Finally, there is the factor of team skill. New techniques tend to be adopted by more skillful teams. But a technique that is more effective for a more skillful team isn’t necessarily going to work for less skillful teams. We’ve seen plenty of cases of less skillful teams building messy monolithic architectures, but it takes time to see what happens when this kind of mess occurs with microservices. A poor team will always create a poor system - it’s very hard to tell if microservices reduce the mess in this case or make it worse. One reasonable argument we’ve heard is that you shouldn’t start with a microservices architecture. Instead begin with a monolith, keep it modular, and split it into microservices once the monolith becomes a problem. (Although this advice isn’t ideal, since a good in-process interface is usually not a good service interface.) So we write this with cautious optimism. So far, we’ve seen enough about the microservice style to feel that it can be a worthwhile road to tread. We can’t say for sure where we’ll end up, but one of the challenges of software development is that you can only make decisions based on the imperfect information that you currently have to hand. Footnotes1: The term “microservice” was discussed at a workshop of software architects near Venice in May, 2011 to describe what the participants saw as a common architectural style that many of them had been recently exploring. In May 2012, the same group decided on “microservices” as the most appropriate name. James presented some of these ideas as a case study in March 2012 at 33rd Degree in Krakow in Microservices - Java, the Unix Way as did Fred George about the same time. Adrian Cockcroft at Netflix, describing this approach as “fine grained SOA” was pioneering the style at web scale as were many of the others mentioned in this article - Joe Walnes, Dan North, Evan Botcher and Graham Tackley. 2: The term monolith has been in use by the Unix community for some time. It appears in The Art of Unix Programming to describe systems that get too big. 3: Many object-oriented designers, including ourselves, use the term service object in the Domain-Driven Design sense for an object that carries out a significant process that isn’t tied to an entity. This is a different concept to how we’re using “service” in this article. Sadly the term service has both meanings and we have to live with the polyseme. 4: We consider an application to be a social construction that binds together a code base, group of functionality, and body of funding. 5: The original paper can be found on Melvyn Conway’s website here 6: At extremes of scale, organisations often move to binary protocols - protobufs for example. Systems using these still exhibit the characteristic of smart endpoints, dumb pipes - and trade off transparency for scale. Most web properties and certainly the vast majority of enterprises don’t need to make this tradeoff - transparency can be a big win. 7: We can’t resist mentioning Jim Webber’s statement that ESB stands for “Egregious Spaghetti Box”. 8: Netflix makes the link explicit - until recently referring to their architectural style as fine-grained SOA. 9: “YAGNI” or “You Aren’t Going To Need It” is an XP principle and exhortation to not add features until you know you need them. 10: It’s a little disengenuous of us to claim that monoliths are single language - in order to build systems on todays web, you probably need to know JavaScript and XHTML, CSS, your server side language of choice, SQL and an ORM dialect. Hardly single language, but you know what we mean. 11: Adrian Cockcroft specifically mentions “developer self-service” and “Developers run what they wrote”(sic) in this excellent presentation delivered at Flowcon in November, 2013. 12: We are being a little disengenuous here. Obviously deploying more services, in more complex topologies is more difficult than deploying a single monolith. Fortunately, patterns reduce this complexity - investment in tooling is still a must though. 13: In fact, Dan North refers to this style as Replaceable Component Architecture rather than microservices. Since this seems to talk to a subset of the characteristics we prefer the latter. 14: Kent Beck highlights this as one his design principles in Implementation Patterns. 15: And SOA is hardly the root of this history. I remember people saying “we’ve been doing this for years” when the SOA term appeared at the beginning of the century. One argument was that this style sees its roots as the way COBOL programs communicated via data files in the earliest days of enterprise computing. In another direction, one could argue that microservices are the same thing as the Erlang programming model, but applied to an enterprise application context.","tags":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]}]